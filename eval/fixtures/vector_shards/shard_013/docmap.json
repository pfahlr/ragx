{
  "documents": [
    {
      "format": "md",
      "id": "PSY0065_Object_permanence",
      "metadata": {
        "source_format": "md",
        "source_path": "/tmp/corpus_split_psychology/shard_013/PSY0065_Object_permanence.md",
        "source_relpath": "PSY0065_Object_permanence.md"
      },
      "path": "PSY0065_Object_permanence.md",
      "text": "# Object_permanence\n\nObject permanence is the understanding that whether an object can be sensed has no effect on whether it continues to exist. This is a fundamental concept studied in the field of developmental psychology, the subfield of psychology that addresses the development of young children's social and mental capacities. There is not yet scientific consensus on when the understanding of object permanence emerges in human development.\nJean Piaget, the Swiss psychologist who first studied object permanence in infants, argued that it is one of an infant's most important accomplishments, as, without this concept, objects would have no separate, permanent existence. In Piaget's theory of cognitive development, infants develop this understanding by the end of the \"sensorimotor stage\", which lasts from birth to about two years of age. Piaget thought that an infant's perception and understanding of the world depended on their motor development, which was required for the infant to link visual, tactile and motor representations of objects. According to this view, it is through touching and handling objects that infants develop object permanence.\n\n\n## Early research\n\nDevelopmental psychologist Jean Piaget conducted experiments that collected behavioral tests on infants. Piaget studied object permanence by observing infants' reactions when a favorite object or toy was presented and then was covered with a blanket or removed from sight. Object permanence is considered to be one of the earliest methods for evaluating working memory. An infant that has started to develop object permanence might reach for the toy or try to grab the blanket off the toy. Infants that have not yet developed this might appear confused. Piaget interpreted these behavioral signs as evidence of a belief that the object had ceased to exist. Reactions of most infants that had already started developing object permanence were of frustration because they knew it existed, but did not know where it was. However, the reaction of infants that had not yet started developing object permanence was more oblivious. If an infant searched for the object, it was assumed that they believed it continued to exist.\nPiaget concluded that some infants are too young to understand object permanence. A lack of object permanence can lead to A-not-B errors, where children reach for a thing at a place where it should not be. Older infants are less likely to make the A-not-B error because they are able to understand the concept of object permanence more than younger infants. However, researchers have found that A-not-B errors do not always show up consistently. They concluded that this type of error might be due to a failure in memory or the fact that infants usually tend to repeat a previous motor behavior.\n\n\n## Stages\n\n\nIn Piaget's formulation, there are six stages of object permanence. These are:\n\n0\u20131 months: Reflex schema stage \u2013 Babies learn how the body can move and work. Vision is blurred and attention spans remain short through infancy. They are not particularly aware of objects to know they have disappeared from sight. However, babies as young as seven minutes old prefer to look at faces. The three primary achievements of this stage are sucking, visual tracking, and hand closure.\n1\u20134 months: Primary circular reactions \u2013 Babies notice objects and start following their movements. They continue to look where an object was, but for only a few moments. They 'discover' their eyes, arms, hands and feet in the course of acting on objects. This stage is marked by responses to familiar images and sounds (including parent's face) and anticipatory responses to familiar events (such as opening the mouth for a spoon). The infant's actions become less reflexive and intentionality emerges.\n4\u20138 months: Secondary circular reactions \u2013 Babies will reach for an object that is partially hidden, indicating knowledge that the whole object is still there. If an object is completely hidden, however, the baby makes no attempt to retrieve it. The infant learns to coordinate vision and comprehension. Actions are intentional, but the child tends to repeat similar actions on the same object. Novel behaviors are not yet imitated.\n8\u201312 months: Coordination of secondary circular reactions \u2013 This is deemed the most important for the cognitive development of the child. At this stage the child understands causality and is goal-directed. The very earliest understanding of object permanence emerges, as the child is now able to retrieve an object when its concealment is observed. This stage is associated with the classic A-not-B error. After successfully retrieving a hidden object at one location (A), the child fails to retrieve it at a second location (B).\n12\u201318 months: Tertiary circular reaction \u2013 The child gains means-end knowledge and is able to solve new problems. The child is now able to retrieve an object when it is hidden several times within their view, but cannot locate it when it is outside their perceptual field.\n18\u201324 months: Invention of new means through mental combination \u2013 The child fully understands object permanence. They will not fall for A-not-B errors. Also, a baby is able to understand the concept of items that are hidden in containers. If a toy is hidden in a matchbox then the matchbox put under a pillow and then, without the child seeing, the toy is slipped out of the matchbox and the matchbox then given to the child, the child will look under the pillow upon discovery that it is not in the matchbox. The child is able to develop a mental image, hold it in mind, and manipulate it to solve problems, including object permanence problems that are not based solely on perception. The child can now reason about where the object may be when invisible displacement occurs.\n\n\n## Contradicting evidence\n\nIn more recent years, the original Piagetian object permanence account has been challenged by a series of infant studies suggesting that much younger infants do have a clear sense that objects exist even when out of sight. Bower showed object permanence in 3-month-olds. This goes against Piaget's coordination of secondary circular reactions stage because infants are not supposed to understand that a completely hidden object still exists until they are eight to twelve months old. The two studies below demonstrate this idea.\nThe first study showed infants a toy car that moved down an inclined track, disappeared behind a screen, and then reemerged at the other end, still on the track. The researchers created a \"possible event\" where a toy mouse was placed behind the tracks but was hidden by the screen as the car rolled by. Then, researchers created an \"impossible event\". In this situation, the toy mouse was placed on the tracks but was secretly removed after the screen was lowered so that the car seemed to go through the mouse. The infants were surprised by the impossible event, which suggests they remembered not only that the toy mouse still existed (object permanence) but also its location. Also in the 1991 study the researchers used an experiment involving two differently sized carrots (one tall and one short) in order to test the infants' response when the carrots would be moved behind a short wall.   The wall was specifically designed to make the short carrot disappear, as well as tested the infants for habituation patterns on the disappearance of the tall carrot behind the wall (impossible event). Infants as young as 3+1\u20442 months displayed greater stimulation toward the impossible event and much more habituation at the possible event. The same was true of the tall carrot in the second experiment.  This research suggests that infants understand more about objects earlier than Piaget proposed.\nThere are primarily four challenges to Piaget's framework:\n\nWhether or not infants without disabilities actually demonstrate object permanence earlier than Piaget claimed.\nThere is disagreement about the relative levels of difficulty posed by the use of various types of covers and by different object positions.\nControversy concerns whether or not perception of object permanence can be achieved or measured without the motor acts that Piaget regarded as essential.\nThe nature of inferences that can be made from the A-not-B error has been challenged. Studies that have contributed to this discussion have examined the contribution of memory limitations, difficulty with spatial localization, and difficulty in inhibiting the motor act of reaching to location A on the A-not-B error.\nOne criticism of Piaget's theory is that culture and education exert stronger influences on a child's development than Piaget maintained. These factors depend on how much practice their culture provides in developmental processes, such as conversational skills.\n\n\n## In animals\n\nExperiments in non-human primates suggest that monkeys can track the displacement of invisible targets, that invisible displacement is represented in the prefrontal cortex, and that development of the frontal cortex is linked to the acquisition of object permanence. Various evidence from human infants is consistent with this. For example, formation of synapses in the frontal cortex peaks during human infancy, and recent experiments using near infrared spectroscopy to gather neuroimaging data from infants suggests that activity in the frontal cortex is associated with successful completion of object permanence tasks.\nHowever, many other types of animals have been shown to have the ability for object permanence. These include dogs, cats, and a few species of birds such as the carrion crow, Eurasian jays and food-storing magpies.  Dogs are able to reach a level of object permanence that allows them to find food after it has been hidden beneath one of two cups and rotated 90\u00b0. Similarly, cats are able to understand object permanence but not to the same extent that dogs can. Cats fail to understand that if they see something go into an apparatus in one direction that it will still be there if the cat tries to enter from another direction. However, while cats did not seem to be quite as good at this \"invisible displacement test\" as dogs are, it is hard to say whether their poorer performance is a true reflection of their abilities or just due to the way in which they have been tested. A longitudinal study found that carrion crows' ability developed gradually, albeit with slight changes in the order of mastery compared to human infants. There was only one task, task 15, that the crows were not able to master. The crows showed perseverative searches at a previously rewarded location (the so-called \"A-not-B error\"). They mastered visible rotational displacements consistently, but failed at more complex invisible rotational displacements. Another study tested the comparison of how long it took food-storing magpies to develop the object permanence necessary for them to be able to live independently. The research suggests that these magpies followed a very similar pattern as human infants while they were developing.\nAmong invertebrates, cuttlefish have been shown to possess object permanence.\n\n\n## In artificial agents\n\nIt has been shown that artificial intelligent agents can be trained to exhibit object permanence. Building such agents revealed an interesting structure. The object permanence task involves several visual and reasoning components, where the most important ones are to detect a visible object, to learn how it moves and to reason about its movement even when it is not visible. Shamsian et al found that object permanence was achieved when the agent had two separate time-sensitive modules, one that tracks visible objects, and a second that decides \"what to track\" when one object occludes or contains the target. Object permanence has further been shown to apply to videos \"in the wild\".\n\n\n## Recent studies\n\nOne of the areas of focus on object permanence has been how physical disabilities (blindness, cerebral palsy and deafness) and intellectual disabilities (Down syndrome, etc.) affect the development of object permanence. In a study that was performed in 1975\u201376, the result shows that the only area where children with intellectual disabilities performed more weakly than children without disabilities was along the lines of social interaction. Other tasks, such as imitation and causality tasks, were performed more weakly by the children without disabilities. However, object permanence was still acquired similarly because it was not related to social interaction.\nSome psychologists believe that \"while object permanence alone may not predict communicative achievement, object permanence along with several other sensorimotor milestones, plays a critical role in, and interacts with, the communicative development of children with severe disabilities\". This was observed in 2006, in a study recognizing where the full mastery of object permanence is one of the milestones that ties into a child's ability to engage in mental representation. Along with the relationship with language acquisition, object permanence is also related to the achievement of self-recognition. This same study also focused specifically on the effects that Down syndrome has on object permanence. They found that the reason why the children that participated were so successful in acquiring object permanence, was due to their social strength in imitation. Along with imitation being a potential factor in the success, another factor that could impact children with Down syndrome could also be the willingness of the child to cooperate.\nOther, more recent studies suggest that the idea of object permanence may not be an innate function of young children. While, in reference to Piaget's theory, it has been established that young children develop object permanence as they age, the question arises: does this occur because of a particular perception that already existed within the minds of these young children? Is object permanence really an inbred response to the neural pathways developing in young minds? Studies suggest that a multitude of variables may be responsible for the development of object permanence rather than a natural talent of infants. Evidence suggests that infants use a variety of cues while studying an object and their perception of the object's permanence can be tested without physically hiding the object. Rather, the object is occluded, slightly obstructed, from the infants view and they are left only other visual cues, such as examining the object from different trajectories. It was also found that the longer an infant focuses on an object may be due to detected discontinuities in their visual field, or the flow of events, with which the infant has become familiar.\n\n\n## See also\n\n",
      "vector_count": 1,
      "vector_offset": 0
    },
    {
      "format": "md",
      "id": "PSY0066_Oneironautics",
      "metadata": {
        "source_format": "md",
        "source_path": "/tmp/corpus_split_psychology/shard_013/PSY0066_Oneironautics.md",
        "source_relpath": "PSY0066_Oneironautics.md"
      },
      "path": "PSY0066_Oneironautics.md",
      "text": "# Oneironautics\n\nOneironautics () refers to the ability to travel within a dream on a conscious basis. Such a traveler in a dream may be called an oneironaut.\n\n\n## Within one's dream\n\n\nA lucid dream is one in which the dreamer is aware that they are dreaming. They are able to exert some or a complete control over the dream's characters, narrative and/or environment. Early references to the phenomenon are found in ancient Greek texts.\n\n\n## Within the dream of another\n\n\nThe idea of one person being able to consciously travel or interact within the dream of another person, known variously as \"dream telepathy\", \"telepathic lucid dreaming\", or \"telepathic dreaming\", has been explored in the realms of science and fantasy fiction; in recent works, such an interaction is often depicted as a computer-mediated psychotherapeutic action, as is the case in The Cell, and Paprika, as well as through the direct intervention of another sleeping person, as in Inception, Dreamscape, and Waking Life. The concept is also included in the fantasy series The Wheel of Time as an ability \"dreamwalkers\" are able to use.\nA trope in such works of fiction explores the ramifications of whether the sleeping protagonist should enter the sleeping brain of another as opposed to allowing another individual to enter one's own brain; the entering of another individual's brain often results in unpleasant surprises, depending upon the mental state of the host or the preparedness of the guest. Roger Zelazny's 1966 sci-fi novella The Dream Master, which applies computer-mediated dream telepathy in a psychotherapeutic setting, focuses on the protagonist's growing struggle to keep his balance as he enters the brain of a fellow psychotherapist who is blind, and subconsciously destructively hungers for the visual stimuli upon which dreams largely depend.\n\n\n## See also\n\n",
      "vector_count": 1,
      "vector_offset": 1
    },
    {
      "format": "md",
      "id": "PSY0067_Operant_conditioning",
      "metadata": {
        "source_format": "md",
        "source_path": "/tmp/corpus_split_psychology/shard_013/PSY0067_Operant_conditioning.md",
        "source_relpath": "PSY0067_Operant_conditioning.md"
      },
      "path": "PSY0067_Operant_conditioning.md",
      "text": "# Operant_conditioning\n\nOperant conditioning, also called instrumental conditioning, is a learning process in which voluntary behaviors are modified by association with the addition (or removal) of reward or aversive stimuli. The frequency or duration of the behavior may increase through reinforcement or decrease through punishment or extinction.\n\n\n## Origins\n\nOperant conditioning originated with Edward Thorndike, whose law of effect theorised that behaviors arise as a result of consequences as satisfying or discomforting. In the 20th century, operant conditioning was studied by behavioral psychologists, who believed that much of mind and behaviour is explained through environmental conditioning. Reinforcements are environmental stimuli that increase behaviors, whereas punishments are stimuli that decrease behaviors. Both kinds of stimuli can be further categorised into positive and negative stimuli, which respectively involve the addition or removal of environmental stimuli.\nOperant conditioning differs from classical conditioning in both mechanism and outcome. While classical conditioning pairs stimuli to produce involuntary, reflexive behaviors (like salivating at food), operant conditioning shapes voluntary behaviors through their consequences. Actions followed by rewards tend to be repeated, while those followed by negative outcomes diminish.\nThe study of animal learning in the 20th century was dominated by the analysis of these two sorts of learning, and they are still at the core of behavior analysis. They have also been applied to the study of social psychology, helping to clarify certain phenomena such as the false consensus effect.\n\n\n## History\n\n\n\n### Thorndike's law of effect\n\nOperant conditioning, sometimes called instrumental learning, was first extensively studied by Edward L. Thorndike (1874\u20131949), who observed the behavior of cats trying to escape from home-made puzzle boxes. A cat could escape from the box by a simple response such as pulling a cord or pushing a pole, but when first constrained, the cats took a long time to get out. With repeated trials ineffective responses occurred less frequently and successful responses occurred more frequently, so the cats escaped more and more quickly. Thorndike generalized this finding in his law of effect, which states that behaviors followed by satisfying consequences tend to be repeated and those that produce unpleasant consequences are less likely to be repeated. In short, some consequences strengthen behavior and some consequences weaken behavior. By plotting escape time against trial number Thorndike produced the first known animal learning curves through this procedure.\nHumans appear to learn many simple behaviors through the sort of process studied by Thorndike, now called operant conditioning. That is, responses are retained when they lead to a successful outcome and discarded when they do not, or when they produce aversive effects. This usually happens without being planned by any \"teacher\", but operant conditioning has been used by parents in teaching their children for thousands of years.\n\n\n### B. F. Skinner\n\nB.F. Skinner (1904\u20131990) is referred to as the Father of operant conditioning, and his work is frequently cited in connection with this topic. His 1938 book \"The Behavior of Organisms: An Experimental Analysis\", initiated his lifelong study of operant conditioning and its application to human and animal behavior. Following the ideas of Ernst Mach, Skinner rejected Thorndike's reference to unobservable mental states such as satisfaction, building his analysis on observable behavior and its equally observable consequences.\nSkinner believed that classical conditioning was too simplistic to be used to describe something as complex as human behavior. Operant conditioning, in his opinion, better described human behavior as it examined causes and effects of intentional behavior.\nTo implement his empirical approach, Skinner invented the operant conditioning chamber, or \"Skinner Box\", in which subjects such as pigeons and rats were isolated and could be exposed to carefully controlled stimuli. Unlike Thorndike's puzzle box, this arrangement allowed the subject to make one or two simple, repeatable responses, and the rate of such responses became Skinner's primary behavioral measure.  Another invention, the cumulative recorder, produced a graphical record from which these response rates could be estimated. These records were the primary data that Skinner and his colleagues used to explore the effects on response rate of various reinforcement schedules. A reinforcement schedule may be defined as \"any procedure that delivers reinforcement to an organism according to some well-defined rule\". The effects of schedules became, in turn, the basic findings from which Skinner developed his account of operant conditioning. He also drew on many less formal observations of human and animal behavior.\nMany of Skinner's writings are devoted to the application of operant conditioning to human behavior.  In 1948 he published Walden Two, a fictional account of a peaceful, happy, productive community organized around his conditioning principles.  In 1957, Skinner published Verbal Behavior, which extended the principles of operant conditioning to language, a form of human behavior that had previously been analyzed quite differently by linguists and others. Skinner defined new functional relationships such as \"mands\" and \"tacts\" to capture some essentials of language, but he introduced no new principles, treating verbal behavior like any other behavior controlled by its consequences, which included the reactions of the speaker's audience.\n\n\n## Concepts and procedures\n\n\n\n### Origins of operant behavior: operant variability\nOperant behavior is said to be \"emitted\"; that is, initially it is not elicited by any particular stimulus. Thus one may ask why it happens in the first place.  The answer to this question is like Darwin's answer to the question of the origin of a \"new\" bodily structure, namely, variation and selection. Similarly, the behavior of an individual varies from moment to moment, in such aspects as the specific motions involved, the amount of force applied, or the timing of the response. Variations that lead to reinforcement are strengthened, and if reinforcement is consistent, the behavior tends to remain stable. However, behavioral variability can itself be altered through the manipulation of certain variables.\n\n\n### Modifying operant behavior: reinforcement and punishment\n\nReinforcement and punishment are the core tools through which operant behavior is modified. These terms are defined by their effect on behavior. \"Positive\" and \"negative\" refer to whether a stimulus was added or removed, respectively. Similarly, \"reinforcement\" and \"punishment\" refer to the future frequency of the behavior. Reinforcement describes a consequence that makes a behavior occur more often in the future, whereas punishment is a consequence that makes a behavior occur less often.\nThere are a total of four consequences:\n\nPositive reinforcement occurs when a behavior (response) results in a desired stimulus being added and increases the frequency of that behavior in the future. Example: if a rat in a Skinner box gets food when it presses a lever, its rate of pressing will go up. Pressing the lever was positively reinforced.\nNegative reinforcement (a.k.a. escape) occurs when a behavior (response) is followed by the removal of an aversive stimulus, thereby increasing the original behavior's frequency. Example: A child is afraid of loud noises at a fireworks display. They put on a pair of headphones, and they can no longer hear the fireworks. The next time the child sees fireworks, they put on a pair of headphones. Putting on headphones was negatively reinforced.\nPositive punishment (also referred to as \"punishment by contingent stimulation\") occurs when a behavior (response) is followed by an aversive stimulus which makes the behavior less likely to occur in the future. Example: A child touches a hot stove and burns his hand. The next time he sees a stove, he does not touch it. Touching the stove was positively punished.\nNegative punishment (penalty) (also called \"punishment by contingent withdrawal\") occurs when a behavior (response) is followed by the removal of a stimulus, and the behavior is less likely to occur in the future. Example: When an employee puts their lunch in a communal refrigerator, it gets stolen before break time. The next time the employee brings a lunch to work, they do not put it in the refrigerator. Putting the lunch in the refrigerator was negatively punished.\nExtinction is a consequence strategy that occurs when a previously reinforced behavior is no longer reinforced with either positive or negative reinforcement. During extinction the behavior becomes less probable. Occasional reinforcement can lead to an even longer delay before behavior extinction due to the learning factor of repeated instances becoming necessary to get reinforcement, when compared with reinforcement being given at each opportunity before extinction.\nA study suggests that tactile feedback, such as haptic vibrations from mobile devices, can function as secondary reinforcers (i.e., learned rewards that acquire reinforcing value through association), strengthening consumer behaviors such as online purchasing.\n\n\n=### Schedules of reinforcement=\nSchedules of reinforcement are rules that control the delivery of reinforcement. The rules specify either the time that reinforcement is to be made available, or the number of responses to be made, or both. Many rules are possible, but the following are the most basic and commonly used\n\nFixed interval schedule: Reinforcement occurs following the first response after a fixed time has elapsed after the previous reinforcement. This schedule yields a \"break-run\" pattern of response; that is, after training on this schedule, the organism typically pauses after reinforcement, and then begins to respond rapidly as the time for the next reinforcement approaches.\nVariable interval schedule: Reinforcement occurs following the first response after a variable time has elapsed from the previous reinforcement. This schedule typically yields a relatively steady rate of response that varies with the average time between reinforcements.\nFixed ratio schedule: Reinforcement occurs after a fixed number of responses have been emitted since the previous reinforcement. An organism trained on this schedule typically pauses for a while after a reinforcement and then responds at a high rate. If the response requirement is low there may be no pause; if the response requirement is high the organism may quit responding altogether.\nVariable ratio schedule: Reinforcement occurs after a variable number of responses have been emitted since the previous reinforcement. This schedule typically yields a very high, persistent rate of response.\nContinuous reinforcement: Reinforcement occurs after each response. Organisms typically respond as rapidly as they can, given the time taken to obtain and consume reinforcement, until they are satiated.\n\n\n=### Factors that alter the effectiveness of reinforcement and punishment=\nThe effectiveness of reinforcement and punishment can be changed. \n\nSatiation/Deprivation: The effectiveness of a positive or \"appetitive\" stimulus will be reduced if the individual has received enough of that stimulus to satisfy his/her appetite. The opposite effect will occur if the individual becomes deprived of that stimulus: the effectiveness of a consequence will then increase. A subject with a full stomach wouldn't feel as motivated as a hungry one.\nImmediacy: An immediate consequence is more effective than a delayed one. If one gives a dog a treat for sitting within five seconds, the dog will learn faster than if the treat is given after thirty seconds.\nContingency: To be most effective, reinforcement should occur consistently after responses and not at other times. Learning may be slower if reinforcement is intermittent, that is, following only some instances of the same response. Responses reinforced intermittently are usually slower to extinguish than are responses that have always been reinforced.\nSize: The size, or amount, of a stimulus often affects its potency as a reinforcer. Humans and animals engage in cost-benefit analysis. If a lever press brings ten food pellets, lever pressing may be learned more rapidly than if a press brings only one pellet.   A pile of quarters from a slot machine may keep a gambler pulling the lever longer than a single quarter.\nMost of these factors serve biological functions.  For example, the process of satiation helps the organism maintain a stable internal environment (homeostasis). When an organism has been deprived of sugar, for example, the taste of sugar is an effective reinforcer. When the organism's blood sugar reaches or exceeds an optimum level the taste of sugar becomes less effective or even aversive.\n\n\n=### Shaping=\n\nShaping is a conditioning method often used in animal training and in teaching nonverbal humans. It depends on operant variability and reinforcement, as described above. The trainer starts by identifying the desired final (or \"target\") behavior. Next, the trainer chooses a behavior that the animal or person already emits with some probability. The form of this behavior is then gradually changed across successive trials by reinforcing behaviors that approximate the target behavior more and more closely. When the target behavior is finally emitted, it may be strengthened and maintained by the use of a schedule of reinforcement.\n\n\n=### Noncontingent reinforcement=\nNoncontingent reinforcement is the delivery of reinforcing stimuli regardless of the organism's behavior. Noncontingent reinforcement may be used in an attempt to reduce an undesired target behavior by reinforcing multiple alternative responses while extinguishing the target response. As no measured behavior is identified as being strengthened, there is controversy surrounding the use of the term noncontingent \"reinforcement\".\n\n\n### Stimulus control of operant behavior\n\nThough initially operant behavior is emitted without an identified reference to a particular stimulus, during operant conditioning operants come under the control of stimuli that are present when behavior is reinforced. Such stimuli are called \"discriminative stimuli.\" A so-called \"three-term contingency\" is the result. That is, discriminative stimuli set the occasion for responses that produce reward or punishment. Example: a rat may be trained to press a lever only when a light comes on; a dog rushes to the kitchen when it hears the rattle of his/her food bag; a child reaches for candy when s/he sees it on a table.\n\n\n=### Discrimination, generalization & context=\nMost behavior is under stimulus control. Several aspects of this may be distinguished: \n\nDiscrimination typically occurs when a response is reinforced only in the presence of a specific stimulus. For example, a pigeon might be fed for pecking at a red light and not at a green light; in consequence, it pecks at red and stops pecking at green.  Many complex combinations of stimuli and other conditions have been studied; for example an organism might be reinforced on an interval schedule in the presence of one stimulus and on a ratio schedule in the presence of another.\nGeneralization is the tendency to respond to stimuli that are similar to a previously trained discriminative stimulus. For example, having been trained to peck at \"red\" a pigeon might also peck at \"pink\", though usually less strongly.\nContext refers to stimuli that are continuously present in a situation, like the walls, tables, chairs, etc. in a room, or the interior of an operant conditioning chamber. Context stimuli may come to control behavior as do discriminative stimuli, though usually more weakly.  Behaviors learned in one context may be absent, or altered, in another.  This may cause difficulties for behavioral therapy, because behaviors learned in the therapeutic setting may fail to occur in other situations.\n\n\n### Behavioral sequences: conditioned reinforcement and chaining\nMost behavior cannot easily be described in terms of individual responses reinforced one by one. The scope of operant analysis is expanded through the idea of behavioral chains, which are sequences of responses bound together by the three-term contingencies defined above.  Chaining is based on the fact, experimentally demonstrated, that a discriminative stimulus not only sets the occasion for subsequent behavior, but it can also reinforce a behavior that precedes it. That is, a discriminative stimulus is also a \"conditioned reinforcer\". For example, the light that sets the occasion for lever pressing may be used to reinforce \"turning around\" in the presence of a noise. This results in the sequence \"noise \u2013 turn-around \u2013 light \u2013 press lever \u2013 food\". Much longer chains can be built by adding more stimuli and responses.\n\n\n### Escape and avoidance\nIn escape learning, a behavior terminates an (aversive) stimulus. For example, shielding one's eyes from sunlight terminates the (aversive) stimulation of bright light in one's eyes.  (This is an example of negative reinforcement, defined above.) Behavior that is maintained by preventing a stimulus is called \"avoidance,\"  as, for example, putting on sun glasses before going outdoors.  Avoidance behavior raises the so-called \"avoidance paradox\", for, it may be asked, how can the non-occurrence of a stimulus serve as a reinforcer? This question is addressed by several theories of avoidance (see below).\nTwo kinds of experimental settings are commonly used: discriminated and free-operant avoidance learning.\n\n\n=### Discriminated avoidance learning=\nA discriminated avoidance experiment involves a series of trials in which a neutral stimulus such as a light is followed by an aversive stimulus such as a shock. After the neutral stimulus appears an operant response such as a lever press prevents or terminate the aversive stimulus. In early trials, the subject does not make the response until the aversive stimulus has come on, so these early trials are called \"escape\" trials. As learning progresses, the subject begins to respond during the neutral stimulus and thus prevents the aversive stimulus from occurring. Such trials are called \"avoidance trials.\" This experiment is said to involve classical conditioning because a neutral CS (conditioned stimulus) is paired with the aversive US (unconditioned stimulus); this idea underlies the two-factor theory of avoidance learning described below.\n\n\n=### Free-operant avoidance learning=\nIn free-operant avoidance a subject periodically receives an aversive stimulus (often an electric shock) unless an operant response is made; the response delays the onset of the shock. In this situation, unlike discriminated avoidance, no prior stimulus signals the shock. Two crucial time intervals determine the rate of avoidance learning. This first is the S-S (shock-shock) interval. This is time between successive shocks in the absence of a response. The second interval is the R-S (response-shock) interval. This specifies the time by which an operant response delays the onset of the next shock. Each time the subject performs the operant response, the R-S interval without shock begins anew.\n\n\n=### Two-process theory of avoidance=\nThis theory was originally proposed in order to explain discriminated avoidance learning, in which an organism learns to avoid an aversive stimulus by escaping from a signal for that stimulus. Two processes are involved: classical conditioning of the signal followed by operant conditioning of the escape response:\na) Classical conditioning of fear. Initially the organism experiences the pairing of a CS with an aversive US. The theory assumes that this pairing creates an association between the CS and the US through classical conditioning and, because of the aversive nature of the US, the CS comes to elicit a conditioned emotional reaction (CER) \u2013 \"fear.\" b) Reinforcement of the operant response by fear-reduction. As a result of the first process, the CS now signals fear; this unpleasant emotional reaction serves to motivate operant responses, and responses that terminate the CS are reinforced by fear termination. The theory does not say that the organism \"avoids\" the US in the sense of anticipating it, but rather that the organism \"escapes\" an aversive internal state that is caused by the CS.\nSeveral experimental findings seem to run counter to two-factor theory. For example, avoidance behavior often extinguishes very slowly even when the initial CS-US pairing never occurs again, so the fear response might be expected to extinguish (see Classical conditioning). Further, animals that have learned to avoid often show little evidence of fear, suggesting that escape from fear is not necessary to maintain avoidance behavior.\n\n\n=### Operant or \"one-factor\" theory=\nSome theorists suggest that avoidance behavior may simply be a special case of operant behavior maintained by its consequences. In this view the idea of \"consequences\" is expanded to include sensitivity to a pattern of events. Thus, in avoidance, the consequence of a response is a reduction in the rate of aversive stimulation. Indeed, experimental evidence suggests that a \"missed shock\" is detected as a stimulus, and can act as a reinforcer. Cognitive theories of avoidance take this idea a step farther. For example, a rat comes to \"expect\" shock if it fails to press a lever and to \"expect no shock\" if it presses it, and avoidance behavior is strengthened if these expectancies are confirmed.\n\n\n### Operant hoarding\nOperant hoarding refers to the observation that rats reinforced in a certain way may allow food pellets to accumulate in a food tray instead of retrieving those pellets. In this procedure, retrieval of the pellets always instituted a one-minute period of extinction during which no additional food pellets were available but those that had been accumulated earlier could be consumed. This finding appears to contradict the usual finding that rats behave impulsively in situations in which there is a choice between a smaller food object right away and a larger food object after some delay. See schedules of reinforcement.\n\n\n## Neurobiological correlates\n\n\nThe first scientific studies identifying neurons that responded in ways that suggested they encode for conditioned stimuli came from work by Mahlon deLong and by R.T. Richardson. They showed that nucleus basalis neurons, which release acetylcholine broadly throughout the cerebral cortex, are activated shortly after a conditioned stimulus, or after a primary reward if no conditioned stimulus exists. These neurons are equally active for positive and negative reinforcers, and have been shown to be related to neuroplasticity in many cortical regions. Evidence also exists that dopamine is activated at similar times. There is considerable evidence that dopamine participates in both reinforcement and aversive learning. Dopamine pathways project much more densely onto frontal cortex regions. Cholinergic projections, in contrast, are dense even in the posterior cortical regions like the primary visual cortex. A study of patients with Parkinson's disease, a condition attributed to the insufficient action of dopamine, further illustrates the role of dopamine in positive reinforcement. It showed that while off their medication, patients learned more readily with aversive consequences than with positive reinforcement. Patients who were on their medication showed the opposite to be the case, positive reinforcement proving to be the more effective form of learning when dopamine activity is high.\nA neurochemical process involving dopamine has been suggested to underlie reinforcement.  When an organism experiences a reinforcing stimulus, dopamine pathways in the brain are activated. This network of pathways \"releases a short pulse of dopamine onto many dendrites, thus broadcasting a global reinforcement signal to postsynaptic neurons.\" This allows recently activated synapses to increase their sensitivity to efferent (conducting outward) signals, thus increasing the probability of occurrence for the recent responses that preceded the reinforcement. These responses are, statistically, the most likely to have been the behavior responsible for successfully achieving reinforcement. But when the application of reinforcement is either less immediate or less contingent (less consistent), the ability of dopamine to act upon the appropriate synapses is reduced.\n\n\n## Questions about the law of effect\n\nA number of observations seem to show that operant behavior can be established without reinforcement in the sense defined above. Most cited is the phenomenon of autoshaping  (sometimes called \"sign tracking\"), in which a stimulus is repeatedly followed by reinforcement, and in consequence the animal begins to respond to the stimulus. For example, a response key is lighted and then food is presented. When this is repeated a few times a pigeon subject begins to peck the key even though food comes whether the bird pecks or not. Similarly, rats begin to handle small objects, such as a lever, when food is presented nearby. Strikingly, pigeons and rats persist in this behavior even when pecking the key or pressing the lever leads to less food (omission training). Another apparent operant behavior that appears without reinforcement is contrafreeloading.\nThese observations and others appear to contradict the law of effect, and they have prompted some researchers to propose new conceptualizations of operant reinforcement  (e.g.)  A more general view is that autoshaping is an instance of classical conditioning; the autoshaping procedure has, in fact, become one of the most common ways to measure classical conditioning.  In this view, many behaviors can be influenced by both classical contingencies (stimulus-response) and operant contingencies (response-reinforcement), and the experimenter's task is to work out how these interact.\n\n\n## Applications\n\nReinforcement and punishment are ubiquitous in human social interactions, and a great many applications of operant principles have been suggested and implemented.\n\n\n### Biological basis\nOperant conditioning bridges the field of neurobiology and the field of psychology. It\u2019s able to do this by demonstrating how exterior behavioral principles correspond with internal processes. While psychologist describe learning in terms of observable behaviors that can be shaped by consequences, neurobiologist share a different sentiment. neurobiologist analyze how these behaviors are underpinned by neural circuits.\nBoth positive and negative reinforcement has been shown to activate a reward system in the brain. this is able to take place because of the release of dopamine in specific areas such as the nucleus accumbens.\n\n\n### Tools\nTools such as point systems, charts of behavior and token economies are principles that are grounded in operant conditioning. These systems function as conditioned reinforcers, which means that they can be exchanged for primary reinforcers such as a tangible reward.\n\n\n### Addiction and dependence\nPositive and negative reinforcement play central roles in the development and maintenance of addiction and drug dependence. An addictive drug is intrinsically rewarding; that is, it functions as a primary positive reinforcer of drug use. The brain's reward system assigns it incentive salience (i.e., it is \"wanted\" or \"desired\"), so as an addiction develops, deprivation of the drug leads to craving.  In addition, stimuli associated with drug use \u2013 e.g., the sight of a syringe, and the location of use \u2013 become associated with the intense reinforcement induced by the drug. These previously neutral stimuli acquire several properties: their appearance can induce craving, and they can become conditioned positive reinforcers of continued use. Thus, if an addicted individual encounters one of these drug cues, a craving for the associated drug may reappear. For example, anti-drug agencies previously used posters with images of drug paraphernalia as an attempt to show the dangers of drug use. However, such posters are no longer used because of the effects of incentive salience in causing relapse upon sight of the stimuli illustrated in the posters.\nIn drug dependent individuals, negative reinforcement occurs when a drug is self-administered in order to alleviate or \"escape\" the symptoms of physical dependence (e.g., tremors and sweating) and/or psychological dependence (e.g., anhedonia, restlessness, irritability, and anxiety) that arise during the state of drug withdrawal.\n\n\n### Animal training\n\nAnimal trainers and pet owners were applying the principles and practices of operant conditioning long before these ideas were named and studied, and animal training still provides one of the clearest and most convincing examples of operant control. Of the concepts and procedures described in this article, a few of the most salient are the following: \n(a) availability of primary reinforcement (e.g. a bag of dog yummies); \n(b) the use of secondary reinforcement, (e.g. sounding a clicker immediately after a desired response, then giving yummy); \n(c) contingency, assuring that reinforcement (e.g. the clicker) follows the desired behavior and not something else; \n(d) shaping,  as in gradually getting a dog to jump higher and higher; \n(e) intermittent reinforcement, as in gradually reducing the frequency of reinforcement to induce persistent behavior without satiation; \n(f) chaining, where a complex behavior is gradually constructed from smaller units.\n\n\n### Applied behavior analysis\n\nApplied behavior analysis (ABA) is the discipline initiated by B. F. Skinner that applies the principles of conditioning to the modification of socially significant human behavior. It uses the basic concepts of conditioning theory, including conditioned stimulus (SC), discriminative stimulus (Sd), response (R), and reinforcing stimulus (Srein or Sr for reinforcers, sometimes Save for aversive stimuli).\nABA practitioners bring these procedures, and many variations and developments of them, to bear on a variety of socially significant behaviors and issues. In many cases, practitioners use operant techniques to develop constructive, socially acceptable behaviors to replace aberrant behaviors.  The techniques of ABA have been effectively applied to such things as early intensive behavioral interventions for autistic children, research on the principles influencing criminal behavior, HIV prevention, conservation of natural resources, education, gerontology, health and exercise, industrial safety, language acquisition, littering, medical procedures, parenting, psychotherapy, seatbelt use, severe mental disorders, sports, substance abuse, phobias, pediatric feeding disorders, and zoo management and care of animals.  Some of these applications are among those described below.\n\n\n### Child behavior \u2013 parent management training\n\nProviding positive reinforcement for appropriate child behaviors is a major focus of parent management training. Typically, parents learn to reward appropriate behavior through social rewards (such as praise, smiles, and hugs) as well as concrete rewards (such as stickers or points towards a larger reward as part of an incentive system created collaboratively with the child). In addition, parents learn to select simple behaviors as an initial focus and reward each of the small steps that their child achieves towards reaching a larger goal (this concept is called \"successive approximations\").\n\n\n### Economics\n\nBoth psychologists and economists have become interested in applying operant concepts and findings to the behavior of humans in the marketplace. An example \nis the analysis of consumer demand, as indexed by the amount of a commodity that is purchased. In economics, the degree to which price influences consumption is called \"the price elasticity of demand.\" Certain commodities are more elastic than others; for example, a change in price of certain foods may have a large effect on the amount bought, while gasoline and other everyday consumables may be less affected by price changes. In terms of operant analysis, such effects may be interpreted in terms of motivations of consumers and the relative value of the commodities as reinforcers.\n\n\n### Gambling \u2013 variable ratio scheduling\n\nAs stated earlier in this article, a variable ratio schedule yields reinforcement after the emission of an unpredictable number of responses.  This schedule typically generates rapid, persistent responding. Slot machines pay off on a variable ratio schedule, and they produce just this sort of persistent lever-pulling behavior in gamblers. The variable ratio payoff from slot machines and other forms of gambling has often been cited as a factor underlying gambling addiction.\n\n\n### Military psychology\n\nHuman beings have an innate resistance to killing and are reluctant to act in a direct, aggressive way towards members of their own species, even to save life.  This resistance to killing has caused infantry to be remarkably inefficient throughout the history of military warfare.\nThis phenomenon was not understood until S.L.A. Marshall (Brigadier General and military historian) undertook interview studies of WWII infantry immediately following combat engagement. Marshall's well-known and controversial book, Men Against Fire; The Problem of Battle Command in Future Wars, revealed that only 15% of soldiers fired their rifles with the purpose of killing in combat. \nFollowing acceptance of Marshall's research by the US Army in 1946, the Human Resources Research Office of the US Army began implementing new training protocols which resemble operant conditioning methods.  Subsequent applications of such methods increased the percentage of soldiers able to kill to around 50% in Korea and over 90% in Vietnam.  Revolutions in training included replacing traditional pop-up firing ranges with three-dimensional, man-shaped, pop-up targets which collapsed when hit.  This provided immediate feedback and acted as positive reinforcement for a soldier's behavior. Other improvements to military training methods have included the timed firing course; more realistic training; high repetitions; praise from superiors; marksmanship rewards; and group recognition. Negative reinforcement includes peer accountability or the requirement to retake courses.  \n\nModern military training conditions mid-brain response to combat pressure by closely simulating actual combat, using mainly Pavlovian classical conditioning and Skinnerian operant conditioning (both forms of behaviorism).Modern marksmanship training is such an excellent example of behaviorism that it has been used for years in the introductory psychology course taught to all cadets at the US Military Academy at West Point as a classic example of operant conditioning. In the 1980s, during a visit to West Point, B.F. Skinner identified modern military marksmanship training as a near-perfect application of operant conditioning.Lt. Col. Dave Grossman states about operant conditioning and US Military training that:It is entirely possible that no one intentionally sat down to use operant conditioning or behavior modification techniques to train soldiers in this area\u2026But from the standpoint of a psychologist who is also a historian and a career soldier, it has become increasingly obvious to me that this is exactly what has been achieved.\n\n\n### Nudge theory\n\nNudge theory (or nudge) is a concept in behavioural science, political theory and economics which argues that indirect suggestions to try to achieve non-forced compliance can influence the motives, incentives and decision making of groups and individuals, at least as effectively \u2013 if not more effectively \u2013 than direct instruction, legislation, or enforcement.\n\n\n### Praise\n\nThe concept of praise as a means of behavioral reinforcement is rooted in B.F. Skinner's model of operant conditioning. Through this lens, praise has been viewed as a means of positive reinforcement, wherein an observed behavior is made more likely to occur by contingently praising said behavior.  Hundreds of studies have demonstrated the effectiveness of praise in promoting positive behaviors, notably in the study of teacher and parent use of praise on child in promoting improved behavior and academic performance, but also in the study of work performance. Praise has also been demonstrated to reinforce positive behaviors in non-praised adjacent individuals (such as a classmate of the praise recipient) through vicarious reinforcement. Praise may be more or less effective in changing behavior depending on its form, content and delivery. In order for praise to effect positive behavior change, it must be contingent on the positive behavior (i.e., only administered after the targeted behavior is enacted), must specify the particulars of the behavior that is to be reinforced, and must be delivered sincerely and credibly.\nAcknowledging the effect of praise as a positive reinforcement strategy, numerous behavioral and cognitive behavioral interventions have incorporated the use of praise in their protocols. The strategic use of praise is recognized as an evidence-based practice in both classroom management and parenting training interventions, though praise is often subsumed in intervention research into a larger category of positive reinforcement, which includes strategies such as strategic attention and behavioral rewards.\nSeveral studies have been done on the effect cognitive-behavioral therapy and operant-behavioral therapy have on different medical conditions. When patients developed cognitive and behavioral techniques that changed their behaviors, attitudes, and emotions; their pain severity decreased. The results of these studies showed an influence of cognitions on pain perception and impact presented explained the general efficacy of Cognitive-Behavioral therapy (CBT) and Operant-Behavioral therapy (OBT).\n\n\n### Video games\n\nThe majority of video games are designed around a compulsion loop, adding a type of positive reinforcement through a variable rate schedule to keep the player playing. This can lead to the pathology of video game addiction.\n\nAs part of a trend in the monetization of video games during the 2010s, some games offered loot boxes as rewards or as items purchasable by real world funds. Boxes contains a random selection of in-game items. The practice has been tied to the same methods that slot machines and other gambling devices dole out rewards, as it follows a variable rate schedule. While the general perception that loot boxes are a form of gambling, the practice is only classified as such in a few countries. However, methods to use those items as virtual currency for online gambling or trading for real world money has created a skin gambling market that is under legal evaluation.\n\n\n### Defensive medicine\nOne of the many reasons proposed for the dramatic costs associated with healthcare is the practice of defensive medicine.  Prabhu reviews the article by Cole and discusses how the responses of two groups of neurosurgeons are classic operant behavior.  One group practice in a state with restrictions on medical lawsuits and the other group with no restrictions.  The group of neurosurgeons were queried anonymously on their practice patterns.  The physicians changed their practice in response to a negative feedback (fear from lawsuit) in the group that practiced in a state with no restrictions on medical lawsuits.\n\n\n## See also\n\n\n\n## References\n\n\n\n## External links\n\n",
      "vector_count": 1,
      "vector_offset": 2
    },
    {
      "format": "md",
      "id": "PSY0068_Osmoreceptor",
      "metadata": {
        "source_format": "md",
        "source_path": "/tmp/corpus_split_psychology/shard_013/PSY0068_Osmoreceptor.md",
        "source_relpath": "PSY0068_Osmoreceptor.md"
      },
      "path": "PSY0068_Osmoreceptor.md",
      "text": "# Osmoreceptor\n\nAn osmoreceptor is a sensory receptor primarily found in the hypothalamus of most homeothermic organisms that detects changes in osmotic pressure. Osmoreceptors can be found in several structures, including two of the circumventricular organs \u2013 the vascular organ of the lamina terminalis, and the subfornical organ. They contribute to osmoregulation, controlling fluid balance in the body. Osmoreceptors are also found in the kidneys where they also modulate osmolality.\n\n\n## Mechanism of activation in humans\n\nOsmoreceptors are located in two of the circumventricular organs \u2014 the vascular organ of lamina terminalis (VOLT) and the subfornical organ. These two circumventricular organs are located along the anteroventral region of the third ventricle, called the AV3V region. Between these two organs is the median preoptic nucleus, which has multiple nerve connections with the two organs, as well as with the supraoptic nuclei and blood pressure control centers in the medulla oblongata. \nThe osmoreceptors have a defined functionality as neurons that are endowed with the ability to detect extracellular fluid osmolarity. Osmoreceptors have aquaporin 4 proteins spanning through their plasma membranes in which water can diffuse, from an area of high to low water concentration. If plasma osmolarity rises above 290 mOsmol/L, then water will move out of the cell due to osmosis, causing the neuroreceptor to shrink in size. Embedded into the cell membrane are stretch inactivated cation channels (SICs), which when the cell shrinks in size, open and allow positively charged ions, such as Na+ and K+ ions to enter the cell. This causes initial depolarisation of the osmoreceptor and activates voltage-gated sodium channel, which through a complex conformational change, allows more sodium ions to enter the neuron, leading to further depolarisation and an action potential to be generated. This action potential travels along the axon of the neuron, and causes the opening of voltage-dependent calcium channels in the axon terminal. This leads to a Ca2+ influx, due to calcium ions diffusing into the neuron along their electrochemical gradient. The calcium ions binds to the synaptotagmin 1 sub-unit of the SNARE protein attached to the arginine-vasopressin (AVP) containing vesicle membrane. This causes the fusion of the vesicle with the neuronal post synaptic membrane. Subsequent release of AVP into the posterior pituitary gland occurs, whereby vasopressin is secreted into the blood stream of the nearby capillaries.\n\n\n## Macula densa\n\nThe macula densa region of the kidney's juxtaglomerular apparatus is another modulator of blood osmolality. The macula densa responds to changes in osmotic pressure through changes in the rate of sodium ion (Na+) flow through the nephron. Decreased Na+ flow stimulates tubuloglomerular feedback to autoregulate, a signal (thought to be regulated by adenosine) sent to the nearby juxtaglomerular cells of the afferent arteriole, causing the juxtaglomerular cells to release the protease renin into circulation. Renin cleaves the zymogen angiotensinogen, always present in plasma as a result of constitutive production in the liver, into a second inactive form, angiotensin I, which is then converted to its active form, angiotensin II, by angiotensin converting enzyme (ACE), which is widely distributed in the small vessels of the body, but particularly concentrated in the pulmonary capillaries of the lungs. Angiotensin II exerts system wide effects, triggering aldosterone release from the adrenal cortex, direct vasoconstriction, and thirst behaviors originating in the hypothalamus. This is commonly known as the renin-angiotensin-aldosterone system.\n\n\n## See also\n\nHypothalamus\nVasopressin\nBaroreceptors\n\n\n## References\n\n\n\n## External links\n\n",
      "vector_count": 1,
      "vector_offset": 3
    },
    {
      "format": "md",
      "id": "PSY0069_Passive-aggressive_behavior",
      "metadata": {
        "source_format": "md",
        "source_path": "/tmp/corpus_split_psychology/shard_013/PSY0069_Passive-aggressive_behavior.md",
        "source_relpath": "PSY0069_Passive-aggressive_behavior.md"
      },
      "path": "PSY0069_Passive-aggressive_behavior.md",
      "text": "# Passive-aggressive_behavior\n\nPassive-aggressive behavior is a communication that in the mind of the speaker is based on a strong, negative emotion such as anger but is expressed using words that do not convey the emotion, including completely avoiding direct communication when it is socially customary. It can be effective to avoid confrontation, rejection, and criticism but can be confusing, annoying, and exasperating to a recipient of the communication due to the discordance between what they hear and what they perceive.\nPassive-aggressive behavior was first defined clinically by Colonel William C. Menninger during World War II in the context of men's reaction to military compliance. Menninger described soldiers who were not openly defiant but expressed their civil disobedience (what he called \"aggressiveness\") by \"passive measures, such as pouting, stubbornness, procrastination, inefficiency, and passive obstructionism\" due to what Menninger saw as an \"immaturity\" and a reaction to \"routine military stress\".\nThe passive\u2013aggressive personality disorder can be described as: A personality trait marked by a pervasive pattern of negative attitudes and characterized by passive, sometimes obstructionist resistance to complying with expectations in interpersonal or occupational situations. This includes behaviors such as condescension, belittling, snubbing, subtly insulting insinuations, contrarianism, procrastination, stubbornness, sabotage, the silent treatment, victim playing, sarcasm, resentment, sullenness, or deliberate/repeated failure to accomplish requested tasks for which one is often explicitly responsible.\nAn outdated definition rejected by the American Psychiatric Association is as follows: Passive-aggressive behavior is characterized by a habitual pattern of non-active resistance to expected work requirements, opposition, sullenness, stubbornness, and negative attitudes in response to requirements for normal performance levels expected by others. Most frequently it occurs in the workplace, where resistance is exhibited by indirect behaviors such as procrastination, forgetfulness, and purposeful inefficiency, especially in reaction to demands by authority figures, but it can also occur in interpersonal contexts.\nIn conflict theory, passive-aggressive behavior can resemble a behavior better described as catty, as it consists of deliberate, active, but carefully veiled hostile acts which are distinctively different in character from the non-assertive style of passive resistance.\nPassive-aggressive behavior at the workplace can lead to conflict and damage team unity and productivity. If ignored, it could result in decreased office efficiency and frustration among workers. If managers are passive-aggressive, it can end up stifling team creativity. Paula De Angelis says, \"It would actually make perfect sense that those promoted to leadership positions might often be those who on the surface appear to be agreeable, diplomatic and supportive, yet who are actually dishonest, backstabbing saboteurs behind the scenes.\"\n\n\n## See also\n\nAdmiration\nCompassion\nEnrichment\nPrudence\n\n\n## References\n\n\n\n## Bibliography\n\n",
      "vector_count": 1,
      "vector_offset": 4
    }
  ]
}