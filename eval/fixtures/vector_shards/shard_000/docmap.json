{
  "documents": [
    {
      "format": "md",
      "id": "PSY0000_Agnosia",
      "metadata": {
        "source_format": "md",
        "source_path": "/tmp/corpus_split_psychology/shard_000/PSY0000_Agnosia.md",
        "source_relpath": "PSY0000_Agnosia.md"
      },
      "path": "PSY0000_Agnosia.md",
      "text": "# Agnosia\n\nAgnosia is a neurological disorder characterized by an inability to process sensory information. Often there is a loss of ability to recognize objects, persons, sounds, shapes, or smells while the specific sense is neither defective nor is there any significant memory loss. It is usually associated with brain injury or neurological illness, particularly after damage to the occipitotemporal border, which is part of the ventral stream.  Agnosia affects only a single modality, such as vision or hearing. More recently, a top-down interruption is considered to cause the disturbance of handling perceptual information.\n\n\n## Types\n\n\n\n### Visual agnosia\nVisual agnosia is a broad category that refers to a deficiency in the ability to recognize visual objects. Visual agnosia can be further subdivided into two different subtypes: apperceptive visual agnosia and associative visual agnosia.\nIndividuals with apperceptive visual agnosia display the ability to see contours and outlines when shown an object, but they experience difficulty if asked to categorize objects. Apperceptive visual agnosia is associated with damage to one hemisphere, specifically damage to the posterior sections of the right hemisphere.\nIn contrast, individuals with associative visual agnosia experience difficulty when asked to name objects. Associative agnosia is associated with damage to both the right and left hemispheres at the occipitotemporal border.  A specific form of associative visual agnosia is known as prosopagnosia.  Prosopagnosia is the inability to recognize faces.  For example, these individuals have difficulty recognizing friends, family and coworkers. However, individuals with prosopagnosia can recognize all other types of visual stimuli.\n\n\n### Speech agnosia\nSpeech agnosia, or auditory verbal agnosia, refers to \"an inability to comprehend spoken words despite intact hearing, speech production and reading ability\". Patients report that they hear sounds being produced, but that the sounds are fundamentally unrecognizable or untranslatable.\n\nEXAMINER: What did you eat for breakfast?\nPATIENT: Breakfast, breakfast, it sounds familiar but it doesn't speak to me. (Obler & Gjerlow 1999:45)\nDespite an inability to process what the speaker is saying, some patients have been reported to recognize certain characteristic information about the speaker's voice (such as being a man or woman).\n\n\n## Causes\n\nAgnosia can result from strokes, dementia, or other neurological disorders. It may also be trauma-induced by a head injury, brain infection, or hereditary.  Additionally, some forms of agnosia may be the result of developmental disorders. Damage causing agnosia usually occurs in either the occipital or parietal lobes of the brain.  Although one modality may be affected, cognitive abilities in other areas are preserved.\nPatients who experience dramatic recovery from blindness experience significant to total agnosia.\nThe effect of damage to the superior temporal sulcus is consistent with several types of neurolinguistic deficiencies, and some contend that agnosia is one of them. The superior temporal sulcus is vital for speech comprehension because the region is highly involved with the lexical interface. According to the 1985 TRACE II Model, the lexical interface associates sound waves (phonemes) with morphological features to produce meaningful words. This association process is accomplished by lateral inhibition/excitement of certain words within an individual's lexicon (vocabulary). For instance, if an experimenter were to say DOG aloud, the utterance would activate and inhibit various words within the subjects lexical interface:\n\nDOG activates 3, and inhibits 0 letters in DOG. \u2013 +3\nDOG activates 2, and inhibits 1 letters in FOG. \u2013 +2\nDOG activates 1, and inhibits 2 letters in DAN. \u2013 +1\nThe consistency of this model to agnosia is shown by evidence that bilateral lesions to the superior temporal sulcus produces 'pure word deafness' (Kussmaul, 1877), or as it is understood today, speech agnosia. Patients with pure word deafness demonstrate the inability to recognize and process speech sounds with normal auditory processing for non-speech sounds below the level of the cortex.\n\n\n## Diagnosis\n\nIn order to assess an individual for agnosia, it must be verified that the individual does not have a loss of sensation, and that both their language abilities and intelligence are intact.  In order for an individual to be diagnosed with agnosia, they must only be experiencing a sensory deficit in a single modality.  To make a diagnosis, the distinction between apperceptive and associative agnosia must be made.  This distinction can be made by having the individual complete copying and matching tasks.  If the individual has a form of apperceptive agnosia they will not be able to match two stimuli that are identical in appearance.  In contrast, if an individual has a form of associative agnosia, they will not be able to match different examples of a stimulus.  For example, an individual who has been diagnosed with associative agnosia in the visual modality would not be able to match pictures of a laptop that is open with a laptop that is closed.\n\n\n### Pure alexia\n\nIndividuals with pure alexia usually have difficulty reading words as well as difficulty with identifying letters.  In order to assess whether an individual has pure alexia, tests of copying and recognition must be performed. An individual with pure alexia should be able to copy a set of words, and should be able to recognize letters.\n\n\n### Prosopagnosia\n\nIndividuals are usually shown pictures of human faces that may be familiar to them such as famous actors, singers, politicians or family members.  The pictures shown to the patient are selected to be age and culture appropriate.  The task involves the examiner asking the individual to name each face.  If the individual cannot name whose face appears in the picture, the examiner may ask a question that would help to recognize the face in the picture.\n\n\n## Treatment\n\nFor all practical purposes, there is no direct cure. Patients may improve if information is presented in other modalities than the damaged one. Different types of therapies can help to reverse the effects of agnosia. In some cases, occupational therapy or speech therapy can improve agnosia, depending on its cause.\nInitially many individuals with a form of agnosia are unaware of the extent to which they have either a perceptual or recognition deficit.  This may be caused by anosognosia which is the lack of awareness of a deficit.  This lack of awareness usually leads to a form of denial and resistance to any form of help or treatment.  There are various methods that can be used which can help the individual recognize the impairment in perception or recognition that they may have.  A patient can be presented with a stimulus to the impaired modality only to help increase their awareness of their deficit.  Alternatively, a task can be broken down into its component parts so that the individual can see each part of the problem caused by the deficit.  Once the individual acknowledges their perceptual or recognition deficit, a form of treatment may be recommended.  There are various forms of treatment such as compensatory strategies with alternate modalities, verbal strategies, alternate cues and organizational strategies.\n\n\n### Verbal strategies\nUsing verbal descriptions may be helpful for individuals with certain types of agnosia.  Individuals such as prosopagnosics may find it useful to listen to a description of their friend or family member and recognize them based on this description more easily than through visual cues.\n\n\n### Alternate cues\nAlternate cues may be particularly useful to an individual with environmental agnosia or prosopagnosia.  Alternate cues for an individual with environmental agnosia may include color cues or tactile markers to symbolize a new room or to remember an area by.  Prosopagnosics may use alternate visual cues such as a scar on an individual's face or crooked teeth, or cues from other senses, like the sound of an individual's voice, in order to recognize the individual. Hair color and length can be helpful cues as well.\n\n\n### Organizational strategies\nOrganizational strategies may be extremely helpful for an individual with visual agnosia.  For example, organizing clothes according to different hangers provides tactile cues for the individual, making it easier to identify certain forms of clothing as opposed to relying solely on visual cues.\n\n\n### Current research\nThere are clinical trials being done to further research for treatments. At the National Institute of Neurological Disorders and Stroke (NINDS) they support research for rare diseases like agnosia. Some organizations that are recruiting for trials are using clincaltrials.gov and give status updates on the trials.\n\n\n## History\n\nThe term \"agnosia\" comes from the Ancient Greek \u1f00\u03b3\u03bd\u03c9\u03c3\u03af\u03b1 (agnosia), \"ignorance\", \"absence of knowledge\". It was introduced by Sigmund Freud in 1891: \"For disturbances in the recognition of objects, which Finkelnburg classes as asymbolia, I should like to propose the term 'agnosia'.\" Prior to Freud's introduction of the term, some of the first ideas about agnosia came from Carl Wernicke, who created theories about receptive aphasia in 1874.  He noted that individuals with receptive aphasia did not possess the ability to understand speech or repeat words.  He believed that receptive aphasia was due to lesions of the posterior third of the left superior temporal gyrus.  Due to these lesions, Wernicke believed that individuals with receptive aphasia had a limited deafness for certain sounds and frequencies in speech.\nAfter Wernicke, came Kussmaul in 1877 who attempted to explain why auditory verbal agnosia, also known as word deafness, occurs.  Contrary to Wernicke's explanations, Kussmaul believed auditory verbal agnosia was the result of major destruction to the first left temporal gyrus.  Kussmaul also posited about the origins of alexia (acquired dyslexia) also known as word blindness.  He believed that word blindness was the result of lesions to the left angular and supramarginal gyri.\nHeinrich Lissauer shared his ideas about agnosia after Wernicke and Kussmaul. In 1890, he theorized that there were two ways in which object recognition impairment could occur.  One way in which impairment could occur was if there was damage to early perceptual processing or if there was damage to the actual object representation.  If the actual object representation was damaged, this would not allow the object to be stored in visual memory, and therefore the individual would not be able to recognize the object.  During the time of Wernicke, Kussmaul and Lissauer there was little known about the cerebral cortex.  Today, with new neuroimaging techniques, we have been able to expand our knowledge on agnosia greatly.\n\n\n## See also\n\nAgnoiology \u2013 Study of ignorance\nPyrrho \u2013 Greek philosopher and founder of Pyrrhonism (c.360-c.270 BC) who suspended judgement of the senses to attain freedom from disturbance\n\n\n## References\n\n\n\n## External links\n\n",
      "vector_count": 1,
      "vector_offset": 0
    },
    {
      "format": "md",
      "id": "PSY0001_AMPA_receptor",
      "metadata": {
        "source_format": "md",
        "source_path": "/tmp/corpus_split_psychology/shard_000/PSY0001_AMPA_receptor.md",
        "source_relpath": "PSY0001_AMPA_receptor.md"
      },
      "path": "PSY0001_AMPA_receptor.md",
      "text": "# AMPA_receptor\n\nThe \u03b1-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid receptor (AMPA receptor, AMPAR, or quisqualate receptor) is an ionotropic glutamate receptor (iGluR) and predominantly sodium ion channel that mediates fast excitatory neurotransmission in the central nervous system (CNS).  Its activation by the neurotransmitter glutamate facilitates rapid neuronal communication, essential for various brain functions, including learning and memory. Its name is derived from the ability to be activated by the artificial glutamate analog AMPA. The receptor was initially named the \"quisqualate receptor\" by Watkins and colleagues after the naturally occurring agonist quisqualate. Later, the receptor was designated as the \"AMPA receptor\" following the development of the selective agonist AMPA by Tage Honore and colleagues at the Royal Danish School of Pharmacy in Copenhagen. The GRIA2-encoded AMPA receptor ligand binding core (GluA2 LBD) was the first glutamate receptor ion channel domain to be crystallized.\n\n\n## Structure and function\n\n\n\n### Subunit composition\nAMPARs are composed of four types of subunits encoded by different genes, designated as GRIA1 (GluA1 or GluR1), GRIA2 (GluA2 or GluR2), GRIA3 (GluA3 or GluR3), and GRIA4 (GluA4 or GluRA-D2), which combine to form a tetrameric structure. Most AMPARs are heterotetrameric, consisting of symmetric 'dimer of dimers' of GluA2 and either GluA1, GluA3 or GluA4.  Dimerization starts in the endoplasmic reticulum with the interaction of N-terminal LIVBP domains, then \"zips up\" through the ligand-binding domain into the transmembrane ion pore.\nThe conformation of the subunit protein in the plasma membrane caused controversy for some time.  While the amino acid sequence of the subunit indicated that there seemed to be four transmembrane protein domains (parts of the protein that pass through the plasma membrane), proteins interacting with the subunit indicated that the N-terminus were extracellular, while the C-terminus were intracellular.  However, if each of the four transmembrane domains went all the way through the plasma membrane, then the two termini would have to be on the same side of the membrane.  It was eventually discovered that the second \"transmembrane\" domain (M2) does not fully traverse the membrane but instead forms a reentrant helix-loop, contributing to the ion-conducting pore of the receptor.  The domain kinks back on itself within the membrane and returns to the intracellular side.  When the four subunits of the tetramer come together, this second membranous domain forms the ion-permeable pore of the receptor.  The M2 loop plays a crucial role in forming the ion channel's selectivity filter, with the helical portions of M2 contributing to hydrophobic interfaces between AMPAR subunits in the ion channel.\nAMPAR subunits differ most in their C-terminal sequence, which determines their interactions with scaffolding proteins.  All AMPARs contain PDZ-binding domains, but which PDZ domain they bind to differs.  For example, GluA1 binds to SAP97 through SAP97's class I PDZ domain, while GluA2 binds to PICK1 and GRIP/ABP.  Of note, AMPARs cannot directly bind to the common synaptic protein PSD-95 owing to incompatible PDZ domains, although they do interact with PSD-95 via stargazin (the prototypical member of the TARP family of AMPAR auxiliary subunits).\nPhosphorylation of AMPARs can regulate channel localization, conductance, and open probability.  GluA1 has four known phosphorylation sites at serine 818 (S818), S831, threonine 840, and S845 (other subunits have similar phosphorylation sites, but GluR1 has been the most extensively studied).  S818 is phosphorylated by protein kinase C (PKC) and is necessary for long-term potentiation (LTP; for GluA1's role in LTP, see below).  S831 is phosphorylated by CaMKII and PKC during LTP, which helps deliver GluA1-containing AMPAR to the synapse, and increases their single channel conductance.  The T840 site was more recently discovered, and has been implicated in LTD.  Finally, S845 is phosphorylated by protein kinase A (PKA) which regulates its open probability.\n\n\n### Mechanism of Action\nAMPA receptors are integral to fast excitatory neurotransmission in the CNS.  Each receptor is a tetramer composed of four subunits, each providing a binding site for agonists like glutamate. The ligand-binding domain is formed by the N-terminal tail and the extracellular loop between transmembrane domains three and four.  The subunit composition significantly influences the receptor's functional properties, including ion permeability and gating kinetics. \n\n\n=### Agonist Binding and Channel Activation=\nUpon glutamate binding, these two loops move towards each other, leading to pore opening.  The channel opens when two sites are occupied, and increases its current as more binding sites are occupied.  This opening allows the influx of sodium (Na\u207a) and, depending on subunit composition, calcium (Ca\u00b2\u207a) ions into the postsynaptic neuron, leading to depolarization and the propagation of excitatory signals. Once open, the channel may undergo rapid desensitization, stopping the current.  \n\n\n=### Desensitization Mechanism=\nThe mechanism of desensitization is due to a small change in angle of one of the parts of the binding site, closing the pore.  AMPARs open and close quickly (1ms), and are thus responsible for most of the fast excitatory postsynaptic transmission in the central nervous system. \n\n\n=### Subunit Composition and Ion Permeability=\nThe AMPAR's permeability to calcium and other cations, such as sodium and potassium, is governed by the GluA2 subunit. If an AMPAR lacks a GluA2 subunit, then it will be permeable to sodium, potassium, and calcium.  The presence of a GluA2 subunit will render the channel impermeable to calcium.  This is determined by post-transcriptional modification \u2014 RNA editing \u2014 of the Q-to-R editing site of the GluA2 mRNA. Here, A\u2192I editing alters the uncharged amino acid glutamine (Q) to the positively charged arginine (R) in the receptor's ion channel.  The positively charged amino acid at the critical point makes it energetically unfavorable for calcium to enter the cell through the pore.  Almost all of the GluA2 subunits in CNS are edited to the GluA2(R) form. This means that the principal ions gated by AMPARs are sodium and potassium, distinguishing AMPARs from NMDA receptors (the other main ionotropic glutamate receptors in the brain), which also permit calcium influx. Both AMPA and NMDA receptors, however, have an equilibrium potential near 0 mV. The prevention of calcium entry into the cell on activation of GluA2-containing AMPARs is proposed to guard against excitotoxicity.\nThe subunit composition of the AMPAR is also important for the way this receptor is modulated.  If an AMPAR lacks GluA2 subunits, then it is susceptible to being blocked in a voltage-dependent manner by a class of molecules called polyamines. Thus, when the neuron is at a depolarized membrane potential, polyamines will block the AMPAR channel more strongly, preventing the flux of potassium ions through the channel pore.  GluA2-lacking AMPARs are, thus, said to have an inwardly rectifying I/V curve, which means that they pass less outward current than inward current at equivalent distance from the reversal potential. Calcium permeable AMPARs are found typically early during postnatal development on neocortical pyramidal neurons, some interneurons, or in dopamine neurons of the ventral tegmental area after the exposure to an addictive drug.\nAlongside RNA editing, alternative splicing allows a range of functional AMPA receptor subunits beyond what is encoded in the genome.  In other words, although one gene (GRIA1\u2013GRIA4) is encoded for each subunit (GluA1\u2013GluA4), splicing after transcription from DNA allows some exons to be translated interchangeably, leading to several functionally different subunits from each gene.\nThe flip/flop sequence is one such interchangeable exon.  A 38-amino acid sequence found prior to (i.e., before the N-terminus of) the fourth membranous domain in all four AMPAR subunits, it determines the speed of desensitization of the receptor and also the speed at which the receptor is resensitized and the rate of channel closing. The flip form is present in prenatal AMPA receptors and gives a sustained current in response to glutamate activation.\n\n\n## Synaptic plasticity\n\nAMPA receptors (AMPAR) are both glutamate receptors and cation channels that are integral to plasticity and synaptic transmission at many postsynaptic membranes. One of the most widely and thoroughly investigated forms of plasticity in the nervous system is known as long-term potentiation (LTP). There are two necessary components of LTP: presynaptic glutamate release and postsynaptic depolarization. Therefore, LTP can be induced experimentally in a paired electrophysiological recording when a presynaptic cell is stimulated to release glutamate on a postsynaptic cell that is depolarized. The typical LTP induction protocol involves a \"tetanus\" stimulation, which is a 100-Hz stimulation for 1 second. When one applies this protocol to a pair of cells, one will see a sustained increase of the amplitude of the excitatory postsynaptic potential (EPSP) following tetanus. This response is interesting because it is thought to be the physiological correlation for learning and memory in the cell. In fact, it has been shown that, following a single paired-avoidance paradigm in mice, LTP can be recorded in some hippocampal synapses in vivo.\nThe molecular basis for LTP has been extensively studied, and AMPARs have been shown to play an integral role in the process. \nBoth GluR1 and GluR2 play an important role in synaptic plasticity. It is now known that the underlying physiological correlation for the increase in EPSP size is a postsynaptic upregulation of AMPARs at the membrane, which is accomplished through the interactions of AMPARs with many cellular proteins.\nThe simplest explanation for LTP is as follows (see the long-term potentiation article for a much more detailed account). Glutamate binds to postsynaptic AMPARs and another glutamate receptor, the NMDA receptor (NMDAR). Ligand binding causes the AMPARs to open, and Na+ flows into the postsynaptic cell, resulting in a depolarization. NMDARs, on the other hand, do not open directly because their pores are occluded at resting membrane potential by Mg2+ ions. NMDARs can open only when a depolarization from the AMPAR activation leads to repulsion of the Mg2+ cation out into the extracellular space, allowing the pore to pass current. Unlike AMPARs, however, NMDARs are permeable to both Na+ and Ca2+. The Ca2+ that enters the cell triggers the upregulation of AMPARs to the membrane, which results in a long-lasting increase in EPSP size underlying LTP.  The calcium entry also phosphorylates CaMKII, which phosphorylates AMPARs, increasing their single-channel conductance.\n\n\n### AMPA receptor trafficking\n\n\n=### Molecular and signaling response to LTP-inducing stimuli=\nThe mechanism for LTP has long been a topic of debate, but, recently, mechanisms have come to some consensus. AMPARs play a key role in this process, as one of the key indicators of LTP induction is the increase in the ratio of AMPAR to NMDARs following high-frequency stimulation. The idea is that AMPARs are trafficked from the dendrite into the synapse and incorporated through some series of signaling cascades.\nAMPARs are initially regulated at the transcriptional level at their 5' promoter regions. There is significant evidence pointing towards the transcriptional control of AMPA receptors in longer-term memory through cAMP response element-binding protein (CREB) and mitogen-activated protein kinases (MAPK). Messages are translated on the rough endoplasmic reticulum (rough ER) and modified there. Subunit compositions are determined at the time of modification at the rough ER. After post-ER processing in the Golgi apparatus, AMPARs are released into the perisynaptic membrane as a reserve waiting for the LTP process to be initiated.\nThe first key step in the process following glutamate binding to NMDARs is the influx of calcium through the NMDA receptors and the resultant activation of Ca2+/calmodulin-dependent protein kinase (CaMKII). Blocking either this influx or the activation of CaMKII prevents LTP, showing that these are necessary mechanisms for LTP. In addition, profusion of CaMKII into a synapse causes LTP, showing that it is a causal and sufficient mechanism.\nCaMKII has multiple modes of activation to cause the incorporation of AMPA receptors into the perisynaptic membrane. CAMKII enzyme is eventually responsible for the development of the actin cytoskeleton of neuronal cells and, eventually, for the dendrite and axon development (synaptic plasticity). The first is direct phosphorylation of synaptic-associated protein 97 (SAP97), a scaffolding protein. First, SAP-97 and Myosin-VI, a motor protein, are bound as a complex to the C-terminus of AMPARs. Following phosphorylation by CaMKII, the complex moves into the perisynaptic membrane. The second mode of activation is through the MAPK pathway. CaMKII activates the Ras proteins, which go on to activate p42/44 MAPK, which drives AMPAR insertion directly into the perisynaptic membrane.\n\n\n=### AMPA receptor trafficking to the PSD in response to LTP=\nOnce AMPA receptors are transported to the perisynaptic region through PKA or SAP97 phosphorylation, receptors are then trafficked to the postsynaptic density (PSD). However, this process of trafficking to the PSD still remains controversial. One possibility is that, during LTP, there is lateral movement of AMPA receptors from perisynaptic sites directly to the PSD.  Another possibility is that exocytosis of intracellular vesicles is responsible for AMPA trafficking to the PSD directly. Recent evidence suggests that both of these processes are happening after an LTP stimulus; however, only the lateral movement of AMPA receptors from the perisynaptic region enhances the number of AMPA receptors at the PSD. The exact mechanism responsible for lateral movement of AMPA receptors to the PSD remains to be discovered; however, research has discovered several essential proteins for AMPA receptor trafficking. For example, overexpression of SAP97 leads to increased AMPA receptor trafficking to synapses. In addition to influencing synaptic localization, SAP97 has also been found to influence AMPA receptor conductance in response to glutamate. Myosin proteins are calcium sensitive motor proteins that have also been found to be essential for AMPA receptor trafficking. Disruption of myosin Vb interaction with Rab11 and Rab11-FIP2 blocks spine growth and AMPA receptor trafficking. Therefore, it is possible that myosin may drive the lateral movement of AMPA receptors in the perisynaptic region to the PSD. Transmembrane AMPA receptor regulatory proteins (TARPs) are a family protein that associate with AMPA receptors and control their trafficking and conductance. CACNG2 (Stargazin) is one such protein and is found to bind AMPA receptors in the perisynaptic and postsynaptic regions. The role of stargazin in trafficking between the perisynaptic and postsynaptic regions remains unclear; however, stargazin is essential for immobilizing AMPA receptors in the PSD by interacting with PSD-95. PSD-95 stabilizes AMPA receptors to the synapse and disruption of the stargazin-PSD-95 interaction suppressed synaptic transmission.\n\n\n=### Biophysics of AMPA receptor trafficking=\nThe movement of AMPA receptors within the neuronal membrane is commonly modeled as Brownian diffusion, reflecting their lateral mobility across the lipid bilayer.  However, at synaptic sites\u2014 particularly the postsynaptic density (PSD)\u2014this motion is modulated by retention forces that can transiently stabilize receptors. These forces do not completely immobilize AMPARs but instead permit a dynamic exchange with receptors in the perisynaptic domain. \nThe molecular basis for this stabilization is believed to involve nanodomain organization within the PSD, including anchoring interactions with scaffolding proteins such as PSD-95 and transmembrane AMPA receptor regulatory proteins (TARPs). Recent evidence suggests that this compartmentalization may arise through liquid-liquid phase separation (LLPS), a biophysical process by which biomolecular condensates form via weak, multivalent interactions.  LLPS may contribute to the formation of synaptic nanodomains that selectively retain or enrich AMPARs at functional sites within the PSD. \n\n\n=### Constitutive trafficking and changes in subunit composition=\nAMPA receptors are continuously being trafficked (endocytosed, recycled, and reinserted) into and out of the plasma membrane. Recycling endosomes within the dendritic spine contain pools of AMPA receptors for such synaptic reinsertion. Two distinct pathways exist for the trafficking of AMPA receptors: a regulated pathway and a constitutive pathway.\nIn the regulated pathway, GluA1-containing AMPA receptors are trafficked to the synapse in an activity-dependent manner, stimulated by NMDA receptor activation. Under basal conditions, the regulated pathway is essentially inactive, being transiently activated only upon the induction of long-term potentiation. This pathway is responsible for synaptic strengthening and the initial formation of new memories.\nIn the constitutive pathway, GluA1-lacking AMPA receptors, usually GluR2-GluR3 heteromeric receptors, replace the GluA1-containing receptors in a one-for-one, activity-independent manner, preserving the total number of AMPA receptors in the synapse. This pathway is responsible for the maintenance of new memories, sustaining the transient changes resulting from the regulated pathway. Under basal conditions, this pathway is routinely active, as it is necessary also for the replacement of damaged receptors.\nThe GluA1 and GluA4 subunits consist of a long carboxy (C)-tail, whereas the GluA2 and GluA3 subunits consist of a short carboxy-tail. The two pathways are governed by interactions between the C termini of the AMPA receptor subunits and synaptic compounds and proteins. Long C-tails prevent GluR1/4 receptors from being inserted directly into the postsynaptic density zone (PSDZ) in the absence of activity, whereas the short C-tails of GluA2/3 receptors allow them to be inserted directly into the PSDZ. The GluA2 C terminus interacts with and binds to N-ethylmaleimide sensitive fusion protein (NSF), which allows for the rapid insertion of GluR2-containing AMPA receptors at the synapse. In addition, GluR2/3 subunits are more stably tethered to the synapse than GluR1 subunits.\n\n\n=### LTD-induced endocytosis of AMPA receptors=\n\nLong-term depression enacts mechanisms to decrease AMPA receptor density in selected dendritic spines, dependent on clathrin and calcineurin and distinct from that of constitutive AMPAR trafficking.  The starting signal for AMPAR endocytosis is an NMDAR-dependent calcium influx from low-frequency stimulation, which in turn activates protein phosphatases PP1 and calcineurin.  However, AMPAR endocytosis has also been activated by voltage-dependent calcium channels, agonism of AMPA receptors, and administration of insulin, suggesting general calcium influx as the cause of AMPAR endocytosis.  Blockage of PP1 did not prevent AMPAR endocytosis, but antagonist application to calcineurin led to significant inhibition of this process.\nCalcineurin interacts with an endocytotic complex at the postsynaptic zone, explaining its effects on LTD.  The complex, consisting of a clathrin-coated pit underneath a section of AMPAR-containing plasma membrane and interacting proteins, is the direct mechanism for reduction of AMPARs, in particular GluR2/GluR3 subunit-containing receptors, in the synapse.  Interactions from calcineurin activate dynamin GTPase activity, allowing the clathrin pit to excise itself from the cell membrane and become a cytoplasmic vesicle.  Once the clathrin coat detaches, other proteins can interact directly with the AMPARs using PDZ carboxyl tail domains; for example, glutamate receptor-interacting protein 1 (GRIP1) has been implicated in intracellular sequestration of AMPARs.  Intracellular AMPARs are subsequently sorted for degradation by lysosomes or recycling to the cell membrane.  For the latter, PICK1 and PKC can displace GRIP1 to return AMPARs to the surface, reversing the effects of endocytosis and LTD. when appropriate.  Nevertheless, the highlighted calcium-dependent, dynamin-mediated mechanism above has been implicated as a key component of LTD. and as such may have applications to further behavioral research.\n\n\n## Role in epileptic seizures\n\nAMPA receptors play a key role in the generation and spread of epileptic seizures. Activation of AMPARs by agonists such as kainic acid, a convulsant that is widely used in epilepsy research, has been shown to induce seizures in both animal models and humans, emphasizing their contribution to epileptogenesis.  Conversely, antagonists targeting AMPARs have demonstrated efficacy in suppressing seizure activity, highlighting their potential as therapeutic agents in epilepsy management.\n\n\n### Molecular target for epilepsy therapy\nThe noncompetitive AMPA receptor antagonists talampanel and perampanel have been demonstrated to have activity in the treatment of adults with partial-onset seizures, indicating that AMPA receptor antagonists represent a potential target for the treatment of epilepsy.\n Perampanel (trade name: Fycompa) received Marketing Authorisation Approval by the European Commission for the treatment of partial epilepsy on July 27, 2012. The drug was approved in the United States by the Food and Drug Administration (FDA) on October 22, 2012. As has been the case for most recently developed AEDs including pregabalin, lacosamide and ezogabine, the FDA recommended that perampanel be classified by the Drug Enforcement Administration (DEA) as a scheduled drug. It has been designated as a Schedule 3 controlled substance.\nDecanoic acid acts as a non-competitive AMPA receptor antagonist at therapeutically relevant concentrations, in a voltage- and subunit-dependent manner, and this is sufficient to explain its antiseizure effects. This direct inhibition of excitatory neurotransmission by decanoic acid in the brain contributes to the anticonvulsant effect of the medium-chain triglyceride ketogenic diet. Decanoic acid and the AMPA receptor antagonist drug perampanel act at separate sites on the AMPA receptor, and so it is possible that they have a cooperative effect at the AMPA receptor, suggesting that perampanel and the ketogenic diet could be synergistic.\nPreclinical research suggests that several derivatives of aromatic amino acids with antiglutamatergic properties including AMPA receptor antagonism and inhibition of glutamate release such as 3,5-dibromo-D-tyrosine and 3,5-dibromo-L-phenylalnine exhibit strong anticonvulsant effect in animal models suggesting use of these compounds as a novel class of antiepileptic drugs.\n\n\n## AMPA Receptors in Disease Beyond Epilepsy\n\nAMPA receptors are essential to excitatory neurotransmission in the CNS.  Beyond their established role in epilepsy, recent research indicates that AMPARs are implicated in various neurological and psychiatric disorders, including excitotoxicity in stroke and neurodegeneration, as well as conditions such as amyotrophic lateral sclerosis (ALS), Alzheimer's disease (AD), Huntington's disease, schizophrenia, and autism spectrum disorders (ASD).\n\n\n### Excitotoxicity in Stroke and Neurodegeneration\nExcessive activation of AMPARs, particularly those lacking the GluA2 subunit, leads to increased calcium permeability, contributing to neuronal injury and death\u2014a phenomenon known as excitotoxity.  This mechanism in involved in acute events such as stroke and in chronic neurodegenerative diseases. For instance, in ALS, motor neurons exhibit elevated levels of calcium-permeable AMPARs, rendering them more susceptible to excitotoxic damage.\n\n\n### Role in ALS, Alzheimer's, and Huntington's Diseases\n\n\n=### ALS=\nMotor neurons in ALS patients express high levels of calcium-permeable AMPARs, which, combined with reduced calcium-buffering capacity, make them vulnerable to excitotoxicity.\n\n\n=### Alzheimer's Disease=\nAlterations in AMPAR trafficking and function have been observed in Alzheimer's disease models.  Dysregulation of the Q/R editing site of the GluA2 subunit affects calcium permeability, influencing dendritic spine morphology and contributing to neurodegeneration and memory deficits.\n\n\n=### Huntington's Disease=\nMutant huntingtin protein disrupts AMPAR-mediated synaptic transmission by impairing receptor trafficking, leading to synaptic dysfunction and neuronal loss in Huntington's disease models.\n\n\n### AMPAR Trafficking Deficits in Schizophrenia and Autism\n\n\n=### Schizophrenia=\nAbnormal N-linked glycosylation of AMPAR subunits has been reported in schizophrenia, suggesting impaired receptor trafficking and synaptic localization, which may underlie glutamatergic dysfunction observed in the disorder.\n\n\n=### Autism Spectrum Disorders (ASD)=\nAlterations in AMPAR trafficking have been implicated in ASD.  Studies indicate that dysregulation of proteins involved in AMPAR trafficking, such as CYFIP1, leads to synaptic dysfunction associated with autism-like behaviors.\n\n\n## Agonists\n\n\n5-Fluorowillardiine \u2013 a synthetic modification of willardiine\nAMPATooltip \u03b1-Amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid \u2013 a synthetic agonist after which the receptor is named\nDomoic acid \u2013 a naturally occurring agonist that causes amnesic shellfish poisoning\nGlutamic acid (glutamate) \u2013 the endogenous agonist\nIbotenic acid \u2013 a naturally occurring agonist found in Amanita muscaria\nQuisqualic acid \u2013 a naturally occurring agonist found in certain species\nWillardiine \u2013 a naturally occurring agonist\n\n\n## Positive allosteric modulators\n\n\n\n## Antagonists\n\nBecampanel\nCNQX\nDasolampanel\nDNQX\nFanapanel (MPQX)\nKaitocephalin\nKynurenic acid \u2013 endogenous ligand\nL-theanine\nNBQX\n3,5-Dibromo-L-phenylalanine, a naturally occurring halogenated derivative of L-phenylalanine\nPerampanel\nSelurampanel\nTezampanel\nZonampanel\n\n\n## Negative allosteric modulators\n\n\nBarbiturates (e.g., pentobarbital, sodium thiopental) \u2013 non-selective\nEthanol \u2013 non-selective\nInhalational anaesthetics (e.g., cyclopropane, enflurane, halothane, isoflurane, sevoflurane) \u2013 non-selective\nGYKI-52466\nIrampanel\nPerampanel\nTalampanel\nPEP1-TGL : GluA1 subunit C-terminus peptide analog that inhibits AMPA receptor incorporation to the postsynaptic density\n\n\n## See also\n\nArc/Arg3.1\n\n\n## References\n\n\n\n## External links\n\n",
      "vector_count": 1,
      "vector_offset": 1
    },
    {
      "format": "md",
      "id": "PSY0002_Amygdala",
      "metadata": {
        "source_format": "md",
        "source_path": "/tmp/corpus_split_psychology/shard_000/PSY0002_Amygdala.md",
        "source_relpath": "PSY0002_Amygdala.md"
      },
      "path": "PSY0002_Amygdala.md",
      "text": "# Amygdala\n\nThe amygdala (; pl.: amygdalae  or amygdalas; also corpus amygdaloideum; Latin from Greek, \u1f00\u03bc\u03c5\u03b3\u03b4\u03ac\u03bb\u03b7, amygdal\u0113, 'almond', 'tonsil') is a paired  nuclear complex present in the cerebral hemispheres of vertebrates. It is considered part of the limbic system. In primates, it is located medially within the temporal lobes. It consists of many nuclei, each made up of further subnuclei. The subdivision most commonly made is into the basolateral, central, cortical, and medial nuclei together with the intercalated cell clusters. The amygdala has a primary role in the processing of memory, decision-making, and emotional responses (including fear, anxiety, and aggression). The amygdala was first identified and named by Karl Friedrich Burdach in 1822.\n\n\n## Structure\n\n\nThirteen nuclei have been identified, each with its own subdivisions and distinct connections to the rest of the brain. The chief nuclei are the basolateral complex, the central nucleus, the cortical nucleus, the medial nucleus, and the intercalated cell clusters. The cortical and medial nuclei connect with the olfactory system and hypothalamus. The central nucleus has extensive projections to the brainstem.\nThe basolateral complex can be further subdivided into the lateral, the basal, and the accessory basal nuclei. It has extensive connections with higher-order cortical areas in the prefrontal, temporal, insular cortices, and the hippocampus. The basolateral complex is surrounded by the intercalated cell net that is inhibitory and projects to a broad variety of areas in the basal forebrain, hypothalamus, and the amygdala.\nThe primate amygdala contains about 32 different types of neuron. \n\n\n### Hemispheric specializations\nThe right and left portions of the amygdala have independent memory systems, but work together to store, encode, and interpret emotion.\nThe right hemisphere of the amygdala is associated with negative emotion. It plays a role in the expression of fear and in the processing of fear-inducing stimuli. Fear conditioning, which occurs when a neutral stimulus acquires aversive properties, occurs within the right hemisphere. When an individual is presented with a conditioned, aversive stimulus, it is processed within the right amygdala, producing an unpleasant or fearful response. This emotional response conditions the individual to avoid fear-inducing stimuli and more importantly, to assess threats in the environment.\nThe right hemisphere is also linked to declarative memory, which consists of facts and information from previously experienced events and must be consciously recalled. It also plays a significant role in the retention of episodic memory. Episodic memory consists of the autobiographical aspects of memory, permitting recall of emotional and sensory experience of an event. This type of memory does not require conscious recall. The right amygdala plays a role in the association of time and places with emotional properties.\nIn one study, electrical stimulations of the right amygdala induced negative emotions, especially fear and sadness. In contrast, stimulation of the left amygdala was able to induce either pleasant (happiness) or unpleasant (fear, anxiety, sadness) emotions. Other evidence suggests that the left amygdala plays a role in the brain's reward system.\n\n\n## Development and sex distinction\n\n\nThe amygdala is one of the best-understood brain regions with regard to differences between the sexes. The amygdala is larger in males than females, in children aged 7 to 11, adult humans, and adult rats.\nThere is considerable growth within the first few years of structural development in both male and female amygdalae. Within this early period, female limbic structures grow at a more rapid pace than the male ones. Amongst female subjects, the amygdala reaches its full growth potential approximately 1.5 years before the peak of male development. The structural development of the male amygdala occurs over a longer period than in women. Because of the early development of female amygdalae, they reach their growth potential sooner than males, whose amygdalae continue to develop. The larger relative size of the male amygdala may be attributed to this extended developmental period.\nHormonal factors may contribute to these sex-specific developmental differences. The amygdala is rich in androgen receptors\u2014nuclear receptors that bind to testosterone. Androgen receptors play a role in the DNA binding that regulates gene expression. Though testosterone is present within the female hormonal systems, women have lower levels of testosterone than men. The abundance of testosterone in the male hormonal system may contribute to development. In addition, the grey matter volume on the amygdala is predicted by testosterone levels, which may also contribute to the increased mass of the male amygdala.\nThere are observable developmental differences between the right and left amygdala. The left amygdala reaches its developmental peak approximately 1.5\u20132 years prior to the right amygdala. Despite the early growth of the left amygdala, the right increases in volume for a longer period of time. The right amygdala is associated with response to fearful stimuli as well as face recognition. For the left amygdala, it is inferred that the early development of it functions to provide infants the ability to detect danger due to its reported responds predominantly to fearful events and faces. In childhood, the amygdala is found to react differently to same-sex versus opposite-sex individuals. This reactivity decreases until a person enters adolescence, where it increases dramatically at puberty.\nOther functional and structural differences between male and female amygdalae have been observed. Subjects' amygdala activation was observed when watching a horror film and subliminal stimuli. The results of the study showed a different lateralization of the amygdala in men and women. Enhanced memory for the film was related to enhanced activity of the left, but not the right, amygdala in women, whereas it was related to enhanced activity of the right, but not the left, amygdala in men. Similarly, a study of decision-making ability in patients with unilateral amygdala damage suggested that men with right (but not left) amygdala damage were more likely to be impaired in decision-making ability, while women with left (but not right) amygdala damage were more likely to be impaired in decision-making ability. One study found evidence that, on average, women tend to retain stronger memories for emotional events than men.\n\n\n## Function\n\n\n\n### Connections\nVariability in amygdala connectivity has been related to a variety of behaviors and outcomes such as fear recognition and social network size. A simple view of the information processing through the amygdala follows as: the amygdala sends projections to the hypothalamus, septal nuclei and BNST (via the amygdalofugal tract), the dorsomedial thalamus (via the amygdalothalamic tract), the nuclei of the trigeminal nerve and the facial nerve, the ventral tegmental area, the locus coeruleus, and the laterodorsal tegmental nucleus.\nThe basolateral amygdala projects to the nucleus accumbens, including the medial shell. Glutamatergic neurons in the basolateral amygdala send projections to the nucleus accumbens shell and core. Activation of these projections drive motivational salience. The ability of these projections to drives incentive salience is dependent upon dopamine receptor D1. The endocannabinoid system that produces lipoid neuromodulators has its specific receptors (CB1) found in amygdalae.\n\nThe medial nucleus is involved in the sense of smell and pheromone-processing. It receives input from the olfactory bulb and olfactory cortex.The lateral amygdalae, which send impulses to the rest of the basolateral complexes and to the centromedial nuclei, receive input from the sensory systems. The centromedial nuclei are the main outputs for the basolateral complexes, and are involved in emotional arousal in rats and cats.\n\n\n### Emotional learning\n\nIn complex vertebrates, including humans, the amygdalae perform primary roles in the formation and storage of memories associated with emotional events. Research indicates that, during fear conditioning, sensory stimuli reach the basolateral complexes of the amygdalae, particularly the lateral nuclei, where they form associations with memories of the stimuli. The association between stimuli and the aversive events they predict may be mediated by long-term potentiation, a sustained enhancement of signaling between affected neurons. There have been studies that show that damage to the amygdala can interfere with memory that is strengthened by emotion. One study examined a patient with bilateral degeneration of the amygdala. He was told a violent story accompanied by matching pictures and was observed based on how much he could recall from the story. The patient had less recollection of the story than patients with functional amygdala, showing that the amygdala has a strong connection with emotional learning.\nEmotional memories are thought to be stored in synapses throughout the brain. Fear memories, for example, are considered to be stored in the neuronal connections from the lateral nuclei to the central nucleus of the amygdalae and the bed nuclei of the stria terminalis (part of the extended amygdala). These connections are not the sole site of fear memories given that the nuclei of the amygdala receive and send information to other brain regions that are important for memory such as the hippocampus. Some sensory neurons project their axon terminals to the central nucleus. The central nuclei are involved in the genesis of many fear responses such as defensive behavior (freezing or escape responses), autonomic nervous system responses (changes in blood pressure and heart rate/tachycardia), neuroendocrine responses (stress-hormone release), etc. Damage to the amygdalae impairs both the acquisition and expression of Pavlovian fear conditioning, a form of classical conditioning of emotional responses. Accumulating evidence has suggested that multiple neuromodulators acting in the amygdala regulates the formation of emotional memories.\nThe amygdalae are also involved in appetitive (positive) conditioning. It seems that distinct neurons respond to positive and negative stimuli, but there is no clustering of these distinct neurons into clear anatomical nuclei. However, lesions of the central nucleus in the amygdala have been shown to reduce appetitive learning in rats. Lesions of the basolateral regions do not exhibit the same effect. Research like this indicates that different nuclei within the amygdala have different functions in appetitive conditioning. Nevertheless, researchers found an example of appetitive emotional learning showing an important role for the basolateral amygdala: The na\u00efve female mice are innately attracted to non-volatile pheromones contained in male-soiled bedding, but not by the male-derived volatiles, become attractive if associated with non-volatile attractive pheromones, which act as unconditioned stimulus in a case of Pavlovian associative learning. In the vomeronasal, olfactory, and emotional systems, Fos (gene family) proteins show that non-volatile pheromones stimulate the vomeronasal system, whereas air-borne volatiles activate only the olfactory system. Thus, the acquired preference for male-derived volatiles reveals an olfactory-vomeronasal associative learning. Moreover, the reward system is differentially activated by the primary pheromones and secondarily attractive odorants. Exploring the primary attractive pheromone activates the basolateral amygdala and the shell of nucleus accumbens but neither the ventral tegmental area nor the orbitofrontal cortex. In contrast, exploring the secondarily attractive male-derived odorants involves activation of a circuit that includes the basolateral amygdala, prefrontal cortex, and ventral tegmental area. Therefore, the basolateral amygdala stands out as the key center for vomeronasal-olfactory associative learning.\n\n\n### Memory modulation\nThe amygdala is also involved in the modulation of memory consolidation. Following any learning event, the long-term memory for the event is not formed instantaneously. Rather, information regarding the event is slowly assimilated into long-term (potentially lifelong) storage over time, possibly via long-term potentiation. Recent studies suggest that the amygdala regulates memory consolidation in other brain regions. Also, fear conditioning, a type of memory that is impaired following amygdala damage, is mediated in part by long-term potentiation. During the consolidation period, the memory can be modulated. In particular, it appears that emotional arousal following the learning event influences the strength of the subsequent memory for that event. Greater emotional arousal following a learning event enhances a person's retention of that event. Experiments have shown that administration of stress hormones to mice immediately after they learn something enhances their retention when they are tested two days later.\nThe amygdala, especially the basolateral nuclei, are involved in mediating the effects of emotional arousal on the strength of the memory for the event, as shown by many laboratories including that of James McGaugh. These laboratories have trained animals on a variety of learning tasks and found that drugs injected into the amygdala after training affect the animals' subsequent retention of the task. These tasks include basic classical conditioning tasks such as inhibitory avoidance, where a rat learns to associate a mild footshock with a particular compartment of an apparatus, and more complex tasks such as spatial or cued water maze, where a rat learns to swim to a platform to escape the water. If a drug that activates the amygdalae is injected into the amygdalae, the animals had better memory for the training in the task.\nAmygdala activity at the time of encoding information correlates with retention for that information. However, this correlation depends on the relative \"emotionalness\" of the information. More emotionally arousing information increases amygdalar activity, and that activity correlates with retention. Amygdala neurons show various types of oscillation during emotional arousal, such as theta activity. These synchronized neuronal events could promote synaptic plasticity (which is involved in memory retention) by increasing interactions between neocortical storage sites and temporal lobe structures involved in declarative memory.\nIn rats, DNA damage was found to increase in the amygdala immediately after exposure to stress. Stress was induced by 30 minutes of restraint or by forced swimming. By seven days after exposure to these stresses, increased DNA damage was no longer detectable in the amygdala, probably because of DNA repair.\nBuddhist monks who do compassion meditation have been shown to modulate their amygdala, along with their temporoparietal junction and insula, during their practice. In an fMRI study, more intensive insula activity was found in expert meditators than in novices.\n\nResearch using Rorschach test blot 03 finds that the number of unique responses to this random figure links to larger sized amygdalae. The researchers note, \"Since previous reports have indicated that unique responses were observed at higher frequency in the artistic population than in the nonartistic normal population, this positive correlation suggests that amygdalar enlargement in the normal population might be related to creative mental activity.\"\n\n\n## Neuropsychological correlates of amygdala activity\n\nEarly research on primates provided explanations as to the functions of the amygdala, as well as a basis for further research. As early as 1888, rhesus monkeys with a lesioned temporal cortex (including the amygdala) were observed to have significant social and emotional deficits. Heinrich Kl\u00fcver and Paul Bucy later expanded upon this same observation by showing that large lesions to the anterior temporal lobe produced noticeable changes, including overreaction to all objects, hypoemotionality, loss of fear, hypersexuality, and hyperorality, a condition in which inappropriate objects are placed in the mouth. Some monkeys also displayed an inability to recognize familiar objects and would approach animate and inanimate objects indiscriminately, exhibiting a loss of fear towards the experimenters. This behavioral disorder was later named Kl\u00fcver\u2013Bucy syndrome accordingly, and later research proved it was specifically due to amygdala lesions. Monkey mothers who had amygdala damage showed a reduction in maternal behaviors towards their infants, often physically abusing or neglecting them. In 1981, researchers found that selective radio frequency lesions of the whole amygdala caused Kl\u00fcver\u2013Bucy syndrome.\n\n\n### Social function\nThe amygdala has a role in social function. Amygdala volume is associated with the size and complexity of social networks. Amygdala volume correlates positively with both the size (the number of contacts) and the complexity (the number of different groups) of social networks. \nThe amygdala is involved in facial recognition and emotional expressions. Its role in analysis of social situations stems specifically from its ability to identify and process changes in facial features, although it does not process the direction of a gaze toward a person.\nThe amygdala processes reactions to violations concerning personal space.\n\n\n### Alcoholism\nThe amygdala appears to play a role in binge drinking, being damaged by repeated episodes of intoxication and withdrawal. Protein kinase C-epsilon in the amygdala is important for regulating behavioral responses to morphine and ethanol and controlling anxiety-like behavior. The protein is involved in controlling the function of other proteins and plays a role in the development of the ability to consume a large amount of ethanol. The duration of chronic alcohol consumption and abstinence may affect dynamic brain network adaptations. When excessive drinking occurs, the amygdala is affected through behavioral changes and reduces the brain's plasticity. Often, when binge drinking or alcoholism occurs, the amygdala is affected and leads to behavior damage. These behavioral damages can be lack of control, inability to conduct oneself in a mature manner, irritability and aggressive behavior, anxiety, depression, personality disorders, excessive drug intake, bipolar disorder, confusion, higher tolerance levels, and inappropriate sexual behaviors with others and self.\n\n\n### Anxiety\nFeelings of anxiety start with an environmental stimulus that provokes stress. This can include various smells, sights, and internal sensations that result in anxiety. The amygdala reacts to this stimuli by preparing to either stand and fight or to turn and run. This response is triggered by the release of adrenaline into the bloodstream; the amygdala sends signals to the paraventricular nucleus of the hypothalamus for the initiation of the HPA axis response. Consequently, blood sugar rises, becoming immediately available to the muscles for quick energy. Shaking may occur in an attempt to return blood to the rest of the body. Long-term changes in amygdala neurons may also increase anxiety after long-term or traumatic stress, led by the action of stress-related hormones within the amygdala. On the flip side, blocking the action of stress hormones in the amygdala reduces anxiety. There may also be a link between the amygdala and anxiety.\nThe central nucleus of the amygdala has direct correlations to the hypothalamus and brainstem\u2014areas directly related to fear and anxiety. This connection is evident from studies of animals that have undergone amygdalae removal.\nThe clusters of the amygdala are activated when an individual expresses feelings of fear or aggression. This occurs because the amygdala is the primary structure of the brain responsible for fight-or-flight response. Anxiety and panic attacks can occur when the amygdala senses environmental stressors that stimulate fight-or-flight response. The amygdala is involved in the expression of conditioned fear. Conditioned fear is the framework used to explain the behavior produced when an originally neutral stimulus is consistently paired with a stimulus that evokes fear. Fear is measured by changes in autonomic activity including increased heart rate, increased blood pressure, as well as in simple reflexes such as flinching or blinking. Studies in 2004 and 2006 showed that normal subjects exposed to images of frightened faces or faces of people from another race will show increased activity of the amygdala, even if that exposure is subliminal. However, the amygdala is not necessary for the processing of fear-related stimuli, since persons in whom it is bilaterally damaged show rapid reactions to fearful faces, even in the absence of a functional amygdala.\nPatient S.M., sometimes referred to as SM-046, is an American woman with exclusive and complete bilateral amygdala destruction since late childhood as a consequence of Urbach\u2013Wiethe disease, and has a physiologically greatly reduced ability to feel fear as a result. First described by scientists in 1994, S.M. has been studied extensively in scientific research; she has helped researchers elucidate the function of the amygdala.\n\n\n### Psychological disorders\nWith advances in neuroimaging technology such as MRI, neuroscientists have made significant findings concerning the amygdala in the human brain. A variety of data shows the amygdala has a substantial role in mental states, and is related to many psychological disorders. Some studies have shown children with anxiety disorders tend to have a smaller left amygdala. In the majority of the cases, there was an association between an increase in the size of the left amygdala with the use of SSRIs (antidepressant medication) or psychotherapy. The left amygdala has been linked to social anxiety disorder, obsessive and compulsive disorders, and post-traumatic stress disorder (PTSD), as well as more broadly to separation and generalized anxiety disorder. Multiple studies have found that the amygdalae may be responsible for the emotional reactions of PTSD patients. One study in particular found that when PTSD patients are shown pictures of faces with fearful expressions, their amygdalae tended to have a higher activation than someone without PTSD.\nIn 2006, researchers observed hyperactivity in the amygdala when patients were shown threatening faces or confronted with frightening situations. Patients with severe social phobia showed a correlation with increased response in the amygdala. Individuals with psychopathy show reduced autonomic responses to instructed fear cues than otherwise healthy individuals. Similarly, depressed patients showed exaggerated left amygdala activity when interpreting emotions for all faces, and especially for fearful faces. This hyperactivity was normalized when patients were administered antidepressant medication.\nIn a 2003 study, subjects with borderline personality disorder showed significantly greater left amygdala activity than normal control subjects. Some borderline patients even had difficulties classifying neutral faces or saw them as threatening. The amygdala has been observed to respond differently in people with bipolar disorder. Amygdala dysfunction during face emotion processing is well-documented in bipolar disorder. Individuals with bipolar disorder showed greater amygdala activity (especially the amygdala/medial-prefrontal-cortex circuit). For people with manic bipolar I disorder, a decreased negative functional connectivity between the orbitofrontal cortex and the amygdala was also observed. A 2003 study found that adult and adolescent bipolar patients tended to have considerably smaller amygdala volumes and somewhat smaller hippocampal volumes. Many studies have also focused on the connections between the amygdala and autism.\n\n\n## See also\n\nAccessory olfactory cortical areas\nAmygdala hijack\nBELBIC\nIntercalated cells of the amygdala\nList of regions in the human brain\nTriune brain\nAmygdalotomy\nAmygdalohippocampectomy\n\n\n## References\n\n\n\n## Further reading\n\nLeDoux JE (2008). \"Amygdala\". Scholarpedia. 3 (4): 2698. Bibcode:2008SchpJ...3.2698L. doi:10.4249/scholarpedia.2698.\n\n\n## External links\n\n",
      "vector_count": 1,
      "vector_offset": 2
    },
    {
      "format": "md",
      "id": "PSY0003_Aphasia",
      "metadata": {
        "source_format": "md",
        "source_path": "/tmp/corpus_split_psychology/shard_000/PSY0003_Aphasia.md",
        "source_relpath": "PSY0003_Aphasia.md"
      },
      "path": "PSY0003_Aphasia.md",
      "text": "# Aphasia\n\nAphasia, also known as dysphasia, is an impairment in a person's ability to comprehend or formulate language because of dysfunction in specific brain regions. The major causes are stroke and head trauma; prevalence is hard to determine, but aphasia due to stroke is estimated to be 0.1\u20130.4% in developed countries. Aphasia can also be the result of brain tumors, epilepsy, autoimmune neurological diseases, brain infections, or neurodegenerative diseases (such as dementias).\nTo be diagnosed with aphasia, a person's language must be significantly impaired in one or more of the four aspects of communication. In the case of progressive aphasia, a noticeable decline in language abilities over a short period of time is required. The four aspects of communication include spoken language production, spoken language comprehension, written language production, and written language comprehension. Impairments in any of these aspects can impact functional communication.\nThe difficulties of people with aphasia can range from occasional trouble finding words, to losing the ability to speak, read, or write; intelligence, however, is unaffected. Expressive language and receptive language can both be affected as well. Aphasia also affects visual language such as sign language. In contrast, the use of formulaic expressions in everyday communication is often preserved. For example, while a person with aphasia, particularly expressive aphasia (Broca's aphasia), may not be able to ask a loved one when their birthday is, they may still be able to sing \"Happy Birthday\". One prevalent deficit in all aphasias is anomia, which is a difficulty in finding the correct word.\nWith aphasia, one or more modes of communication in the brain have been damaged and are therefore functioning incorrectly. Aphasia is not caused by damage to the brain resulting in motor or sensory deficits, thus producing abnormal speech \u2014 that is, aphasia is not related to the mechanics of speech, but rather the individual's language cognition. However, it is possible for a person to have both problems, e.g. in the case of a hemorrhage damaging a large area of the brain. An individual's language abilities incorporate the socially shared set of rules, as well as the thought processes that go behind communication (as it affects both verbal and nonverbal language). Aphasia is not a result of other peripheral motor or sensory difficulty, such as paralysis affecting the speech muscles, or a general hearing impairment.\nNeurodevelopmental forms of auditory processing disorder (APD) are differentiable from aphasia in that aphasia is by definition caused by acquired brain injury, but acquired epileptic aphasia has been viewed as a form of APD.\n\n\n## Signs and symptoms\n\nPeople with aphasia may experience any of the following behaviors due to an acquired brain injury, although some of these symptoms may be due to related or concomitant problems, such as dysarthria or apraxia, and not primarily due to aphasia. Aphasia symptoms can vary based on the location of damage in the brain. Signs and symptoms may or may not be present in individuals with aphasia and may vary in severity and level of disruption to communication. Often those with aphasia may have a difficulty with naming objects, so they might use words such as thing or point at the objects. When asked to name a pencil they may say it is a \"thing used to write\".\n\n\n### Related behaviors\nGiven the previously stated signs and symptoms, the following behaviors are often seen in people with aphasia as a result of attempted compensation for incurred speech and language deficits:\n\nSelf-repairs: Further disruptions in fluent speech as a result of mis-attempts to repair erred speech production.\nStruggle in non-fluent aphasias: A severe increase in expelled effort to speak after a life where talking and communicating was an ability that came so easily can cause visible frustration.\nPreserved and automatic language: A behavior in which some language or language sequences that were used frequently prior to onset are still produced with more ease than other language post onset.\n\n\n=### Subcortical=\nSubcortical aphasia's characteristics and symptoms depend upon the site and size of subcortical lesion. Possible sites of lesions include the thalamus, internal capsule, and basal ganglia.\n\n\n### Cognitive deficits\nWhile aphasia has traditionally been described in terms of language deficits, there is increasing evidence that many people with aphasia commonly experience co-occurring non-linguistic cognitive deficits in areas such as attention, memory, executive functions and learning. By some accounts, cognitive deficits, such as attention and working memory constitute the underlying cause of language impairment in people with aphasia. Others suggest that cognitive deficits often co-occur, but are comparable to cognitive deficits in stroke patients without aphasia and reflect general brain dysfunction following injury. Whilst it has been shown that cognitive neural networks support language reorganisation after stroke,\nThe degree to which deficits in attention and other cognitive domains underlie language deficits in aphasia is still unclear.\nIn particular, people with aphasia often demonstrate short-term and working memory deficits. These deficits can occur in both the verbal domain as well as the visuospatial domain. Furthermore, these deficits are often associated with performance on language specific tasks such as naming, lexical processing, and sentence comprehension, and discourse production. Other studies have found that most, but not all people with aphasia demonstrate performance deficits on tasks of attention, and their performance on these tasks correlate with language performance and cognitive ability in other domains. Even patients with mild aphasia, who score near the ceiling on tests of language often demonstrate slower response times and interference effects in non-verbal attention abilities.\nIn addition to deficits in short-term memory, working memory, and attention, people with aphasia can also demonstrate deficits in executive function. For instance, people with aphasia may demonstrate deficits in initiation, planning, self-monitoring, and cognitive flexibility. Other studies have found that people with aphasia demonstrate reduced speed and efficiency during completion of executive function assessments.\nRegardless of their role in the underlying nature of aphasia, cognitive deficits have a clear role in the study and rehabilitation of aphasia. For instance, the severity of cognitive deficits in people with aphasia has been associated with lower quality of life, even more so than the severity of language deficits. Furthermore, cognitive deficits may influence the learning process of rehabilitation and language treatment outcomes in aphasia. Non-linguistic cognitive deficits have also been the target of interventions directed at improving language ability, though outcomes are not definitive. While some studies have demonstrated language improvement secondary to cognitively-focused treatment, others have found little evidence that the treatment of cognitive deficits in people with aphasia has an influence on language outcomes.\nOne important caveat in the measurement and treatment of cognitive deficits in people with aphasia is the degree to which assessments of cognition rely on language abilities for successful performance. Most studies have attempted to circumvent this challenge by utilizing non-verbal cognitive assessments to evaluate cognitive ability in people with aphasia. However, the degree to which these tasks are truly \"non-verbal\" and not mediated by language is unclear. For instance, Wall et al. found that language and non-linguistic performance was related, except when non-linguistic performance was measured by \"real life\" cognitive tasks.\n\n\n## Causes\n\nAphasia is most often caused by stroke, where about a quarter of patients who experience an acute stroke develop aphasia. However, any disease or damage to the parts of the brain that control language can cause aphasia. Some of these can include brain tumors, traumatic brain injury, epilepsy and progressive neurological disorders. In rare cases, aphasia may also result from herpesviral encephalitis. The herpes simplex virus affects the frontal and temporal lobes, subcortical structures, and the hippocampal tissue, which can trigger aphasia. In acute disorders, such as head injury or stroke, aphasia usually develops quickly. When caused by brain tumor, infection, or dementia, it develops more slowly.\nSubstantial damage to tissue anywhere within the region shown in blue (on the figure in the infobox above) can potentially result in aphasia. Aphasia can also sometimes be caused by damage to subcortical structures deep within the left hemisphere, including the thalamus, the internal and external capsules, and the caudate nucleus of the basal ganglia. The area and extent of brain damage or atrophy will determine the type of aphasia and its symptoms. A very small number of people can experience aphasia after damage to the right hemisphere only. It has been suggested that these individuals may have had an unusual brain organization prior to their illness or injury, with perhaps greater overall reliance on the right hemisphere for language skills than in the general population.\nPrimary progressive aphasia (PPA), while its name can be misleading, is actually a form of dementia that has some symptoms closely related to several forms of aphasia. It is characterized by a gradual loss in language functioning while other cognitive domains are mostly preserved, such as memory and personality. PPA usually initiates with sudden word-finding difficulties in an individual and progresses to a reduced ability to formulate grammatically correct sentences (syntax) and impaired comprehension. The etiology of PPA is not due to a stroke, traumatic brain injury (TBI), or infectious disease; it is still uncertain what initiates the onset of PPA in those affected by it.\nEpilepsy can also include transient aphasia as a prodromal or episodic symptom. However, the repeated seizure activity within language regions may also lead to chronic, and progressive aphasia. Aphasia is also listed as a rare side-effect of the fentanyl patch, an opioid used to control chronic pain.\n\n\n## Diagnosis\n\n\n\n### Neuroimaging methods\nMagnetic resonance imaging (MRI) and functional magnetic resonance imaging (fMRI) are the most common neuroimaging tools used in identifying aphasia and studying the extent of damage in the loss of language abilities. This is done by doing MRI scans and locating the extent of lesions or damage within brain tissue, particularly within areas of the left frontal and temporal regions- where a lot of language related areas lie. In fMRI studies a language related task is often completed and then the BOLD image is analyzed. If there are lower than normal BOLD responses that indicate a lessening of blood flow to the affected area and can show quantitatively that the cognitive task is not being completed.\nThere are limitations to the use of fMRI in aphasic patients particularly. Because a high percentage of aphasic patients develop it because of stroke there can be infarct present which is the total loss of blood flow. This can be due to the thinning of blood vessels or the complete blockage of it. This is important in fMRI as it relies on the BOLD response (the oxygen levels of the blood vessels), and this can create a false hyporesponse upon fMRI study. Due to the limitations of fMRI such as a lower spatial resolution, it can show that some areas of the brain are not active during a task when they in reality are. Additionally, with stroke being the cause of many cases of aphasia the extent of damage to brain tissue can be difficult to quantify therefore the effects of stroke brain damage on the functionality of the patient can vary.\n\nNeural substrates of aphasia subtypes\nMRI is often used to predict or confirm the subtype of aphasia present. Researchers compared three subtypes of aphasia \u2014 nonfluent-variant primary progressive aphasia (nfPPA), logopenic-variant primary progressive aphasia (lvPPA), and semantic-variant primary progressive aphasia (svPPA), with primary progressive aphasia (PPA) and Alzheimer's disease. This was done by analyzing the MRIs of patients with each of the subsets of PPA. Images which compare subtypes of aphasia as well as for finding the extent of lesions are generated by overlapping images of different participant's brains (if applicable) and isolating areas of lesions or damage using third-party software such as MRIcron. MRI has also been used to study the relationship between the type of aphasia developed and the age of the person with aphasia. It was found that patients with fluent aphasia are on average older than people with non-fluent aphasia. It was also found that among patients with lesions confined to the anterior portion of the brain an unexpected portion of them presented with fluent aphasia and were remarkably older than those with non-fluent aphasia. This effect was not found when the posterior portion of the brain was studied.\n\nAssociated conditions\nIn a study on the features associated with different disease trajectories in Alzheimer's disease (AD)-related primary progressive aphasia (PPA), it was found that metabolic patterns via PET SPM analysis can help predict progression of total loss of speech and functional autonomy in AD and PPA patients. This was done by comparing an MRI or CT image of the brain and presence of a radioactive biomarker with normal levels in patients without Alzheimer's Disease. Apraxia is another disorder often correlated with aphasia. This is due to a subset of apraxia which affects speech. Specifically, this subset affects the movement of muscles associated with speech production, apraxia and aphasia are often correlated due to the proximity of neural substrates associated with each of the disorders. Researchers concluded that there were 2 areas of lesion overlap between patients with apraxia and aphasia, the anterior temporal lobe and the left inferior parietal lobe.\n\nTreatment and neuroimaging\nEvidence for positive treatment outcomes can also be quantified using neuroimaging tools. The use of fMRI and an automatic classifier can help predict language recovery outcomes in stroke patients with 86% accuracy when coupled with age and language test scores. The stimuli tested were sentences both correct and incorrect and the subject had to press a button whenever the sentence was incorrect. The fMRI data collected focused on responses in regions of interest identified by healthy subjects.  Recovery from aphasia can also be quantified using diffusion tensor imaging. The accurate fasciculus (AF) connects the right and left superior temporal lobe, premotor regions/posterior inferior frontal gyrus. and the primary motor cortex. In a study which enrolled patients in a speech therapy program, an increase in AF fibers and volume was found in patients after 6-weeks in the program which correlated with long-term improvement in those patients. The results of the experiment are pictured in Figure 2. This implies that DTI can be used to quantify the improvement in patients after speech and language treatment programs are applied.\n\n\n### Classification\nAphasia is best thought of as a collection of different disorders, rather than a single problem. Each individual with aphasia will present with their own particular combination of language strengths and weaknesses. Consequently, it is a major challenge just to document the various difficulties that can occur in different people, let alone decide how they might best be treated. Most classifications of the aphasias tend to divide the various symptoms into broad classes. A common approach is to distinguish between the fluent aphasias (where speech remains fluent, but content may be lacking, and the person may have difficulties understanding others), and the nonfluent aphasias (where speech is very halting and effortful, and may consist of just one or two words at a time).\nHowever, no such broad-based grouping has proven fully adequate, or reliable. There is wide variation among people even within the same broad grouping, and aphasias can be highly selective. For instance, people with naming deficits (anomic aphasia) might show an inability only for naming buildings, or people, or colors. Unfortunately, assessments that characterize aphasia in these groupings have persisted. This is not helpful to people living with aphasia, and provides inaccurate descriptions of an individual pattern of difficulties.\nThere are typical difficulties with speech and language that come with normal aging as well. As we age, language can become more difficult to process, resulting in a slowing of verbal comprehension, reading abilities and more likely word finding difficulties. With each of these, though, unlike some aphasias, functionality within daily life remains intact.\n\n\n=### Boston classification=\n\nIndividuals with receptive aphasia (Wernicke's aphasia), also referred to as fluent aphasia, may speak in long sentences that have no meaning, add unnecessary words, and even create new \"words\" (neologisms). For example, someone with receptive aphasia may say, \"delicious taco\", meaning \"The dog needs to go out so I will take him for a walk\". They have poor auditory and reading comprehension, and fluent, but nonsensical, oral and written expression. Individuals with receptive aphasia usually have great difficulty understanding the speech of both themselves and others and are, therefore, often unaware of their mistakes. Receptive language deficits usually arise from lesions in the posterior portion of the left hemisphere at or near Wernicke's area. It is often the result of trauma to the temporal region of the brain, specifically damage to Wernicke's area. Trauma can be the result from an array of problems, however it is most commonly seen as a result of stroke\nIndividuals with expressive aphasia (Broca's aphasia) frequently speak short, meaningful phrases that are produced with great effort. It is thus characterized as a nonfluent aphasia. Affected people often omit small words such as \"is\", \"and\", and \"the\". For example, a person with expressive aphasia may say, \"walk dog\", which could mean \"I will take the dog for a walk\", \"you take the dog for a walk\" or even \"the dog walked out of the yard.\" Individuals with expressive aphasia are able to understand the speech of others to varying degrees. Because of this, they are often aware of their difficulties and can become easily frustrated by their speaking problems. While Broca's aphasia may appear to be solely an issue with language production, evidence suggests that it may be rooted in an inability to process syntactical information. Individuals with expressive aphasia may have a speech automatism (also called recurring or recurrent utterance). These speech automatisms can be repeated lexical speech automatisms; e.g., modalisations ('I can't ..., I can't ...'), expletives/swearwords, numbers ('one two, one two') or non-lexical utterances made up of repeated, legal, but meaningless, consonant-vowel syllables (e.g.., /tan tan/, /bi bi/). In severe cases, the individual may be able to utter only the same speech automatism each time they attempt speech.\nIndividuals with anomic aphasia have difficulty with naming. People with this aphasia may have difficulties naming certain words, linked by their grammatical type (e.g., difficulty naming verbs and not nouns) or by their semantic category (e.g., difficulty naming words relating to photography, but nothing else) or a more general naming difficulty. People tend to produce grammatic, yet empty, speech. Auditory comprehension tends to be preserved. Anomic aphasia is the aphasial presentation of tumors in the language zone; it is the aphasial presentation of Alzheimer's disease. Anomic aphasia is the mildest form of aphasia, indicating a likely possibility for better recovery.\nIndividuals with transcortical sensory aphasia, in principle the most general and potentially among the most complex forms of aphasia, may have similar deficits as in receptive aphasia, but their repetition ability may remain intact.\nGlobal aphasia is considered a severe impairment in many language aspects since it impacts expressive and receptive language, reading, and writing. Despite these many deficits, there is evidence that has shown individuals benefited from speech language therapy. Even though individuals with global aphasia will not become competent speakers, listeners, writers, or readers, goals can be created to improve the individual's quality of life. Individuals with global aphasia usually respond well to treatment that includes personally relevant information, which is also important to consider for therapy.\nIndividuals with conduction aphasia have deficits in the connections between the speech-comprehension and speech-production areas. This might be caused by damage to the arcuate fasciculus, the structure that transmits information between Wernicke's area and Broca's area. Similar symptoms, however, can be present after damage to the insula or to the auditory cortex. Auditory comprehension is near normal, and oral expression is fluent with occasional paraphasic errors. Paraphasic errors include phonemic/literal or semantic/verbal. Repetition ability is poor. Conduction and transcortical aphasias are caused by damage to the white matter tracts. These aphasias spare the cortex of the language centers, but instead create a disconnection between them. Conduction aphasia is caused by damage to the arcuate fasciculus. The arcuate fasciculus is a white matter tract that connects Broca's and Wernicke's areas. People with conduction aphasia typically have good language comprehension, but poor speech repetition and mild difficulty with word retrieval and speech production. People with conduction aphasia are typically aware of their errors. Two forms of conduction aphasia have been described: reproduction conduction aphasia (repetition of a single relatively unfamiliar multisyllabic word) and repetition conduction aphasia (repetition of unconnected short familiar words.\nTranscortical aphasias include transcortical motor aphasia, transcortical sensory aphasia, and mixed transcortical aphasia. People with transcortical motor aphasia typically have intact comprehension and awareness of their errors, but poor word finding and speech production. People with transcortical sensory and mixed transcortical aphasia have poor comprehension and unawareness of their errors. Despite poor comprehension and more severe deficits in some transcortical aphasias, small studies have indicated that full recovery is possible for all types of transcortical aphasia.\n\n\n=### Classical-localizationist approaches=\n\nLocalizationist approaches aim to classify the aphasias according to their major presenting characteristics and the regions of the brain that most probably gave rise to them. Inspired by the early work of nineteenth-century neurologists Paul Broca and Carl Wernicke, these approaches identify two major subtypes of aphasia and several more minor subtypes:\n\nExpressive aphasia (also known as \"motor aphasia\" or \"Broca's aphasia\"), which is characterized by halted, fragmented, effortful speech, but well-preserved comprehension relative to expression. Damage is typically in the anterior portion of the left hemisphere, most notably Broca's area. Individuals with Broca's aphasia often have right-sided weakness or paralysis of the arm and leg, because the left frontal lobe is also important for body movement, particularly on the right side.\nReceptive aphasia (also known as \"sensory aphasia\" or \"Wernicke's aphasia\"), which is characterized by fluent speech, but marked difficulties understanding words and sentences. Although fluent, the speech may lack in key substantive words (nouns, verbs, adjectives), and may contain incorrect words or even nonsense words. This subtype has been associated with damage to the posterior left temporal cortex, most notably Wernicke's area. These individuals usually have no body weakness, because their brain injury is not near the parts of the brain that control movement.\nConduction aphasia, where speech remains fluent, and comprehension is preserved, but the person may have disproportionate difficulty repeating words or sentences. Damage typically involves the arcuate fasciculus and the left parietal region.\nTranscortical motor aphasia and transcortical sensory aphasia, which are similar to Broca's and Wernicke's aphasia respectively, but the ability to repeat words and sentences is disproportionately preserved.\nRecent classification schemes adopting this approach, such as the Boston-Neoclassical Model, also group these classical aphasia subtypes into two larger classes: the nonfluent aphasias (which encompasses Broca's aphasia and transcortical motor aphasia) and the fluent aphasias (which encompasses Wernicke's aphasia, conduction aphasia and transcortical sensory aphasia). These schemes also identify several further aphasia subtypes, including: anomic aphasia, which is characterized by a selective difficulty finding the names for things; and global aphasia, where both expression and comprehension of speech are severely compromised.\nMany localizationist approaches also recognize the existence of additional, more \"pure\" forms of language disorder that may affect only a single language skill. For example, in pure alexia, a person may be able to write, but not read, and in pure word deafness, they may be able to produce speech and to read, but not understand speech when it is spoken to them.\n\n\n=### Cognitive neuropsychological approaches=\nAlthough localizationist approaches provide a useful way of classifying the different patterns of language difficulty into broad groups, one problem is that most individuals do not fit neatly into one category or another. Another problem is that the categories, particularly the major ones such as Broca's and Wernicke's aphasia, still remain quite broad and do not meaningfully reflect a person's difficulties. Consequently, even amongst those who meet the criteria for classification into a subtype, there can be enormous variability in the types of difficulties they experience.\nInstead of categorizing every individual into a specific subtype, cognitive neuropsychological approaches aim to identify the key language skills or \"modules\" that are not functioning properly in each individual. A person could potentially have difficulty with just one module, or with a number of modules. This type of approach requires a framework or theory as to what skills/modules are needed to perform different kinds of language tasks. For example, the model of Max Coltheart identifies a module that recognizes phonemes as they are spoken, which is essential for any task involving recognition of words. Similarly, there is a module that stores phonemes that the person is planning to produce in speech, and this module is critical for any task involving the production of long words or long strings of speech. Once a theoretical framework has been established, the functioning of each module can then be assessed using a specific test or set of tests. In the clinical setting, use of this model usually involves conducting a battery of assessments, each of which tests one or a number of these modules. Once a diagnosis is reached as to the skills/modules where the most significant impairment lies, therapy can proceed to treat these skills.\n\n\n=### Progressive aphasias=\nPrimary progressive aphasia (PPA) is a neurodegenerative focal dementia that can be associated with progressive illnesses or dementia, such as frontotemporal dementia / Pick Complex Motor neuron disease, Progressive supranuclear palsy, and Alzheimer's disease, which is the gradual process of progressively losing the ability to think. Gradual loss of language function occurs in the context of relatively well-preserved memory, visual processing, and personality until the advanced stages. Symptoms usually begin with word-finding problems (naming) and progress to impaired grammar (syntax) and comprehension (sentence processing and semantics). The loss of language before the loss of memory differentiates PPA from typical dementias. People with PPA may have difficulties comprehending what others are saying. They can also have difficulty trying to find the right words to make a sentence. There are three classifications of Primary Progressive Aphasia : Progressive nonfluent aphasia (PNFA), Semantic Dementia (SD), and Logopenic progressive aphasia (LPA).\nProgressive Jargon Aphasia is a fluent or receptive aphasia in which the person's speech is incomprehensible, but appears to make sense to them. Speech is fluent and effortless with intact syntax and grammar, but the person has problems with the selection of nouns. Either they will replace the desired word with another that sounds or looks like the original one or has some other connection or they will replace it with sounds. As such, people with jargon aphasia often use neologisms, and may perseverate if they try to replace the words they cannot find with sounds. Substitutions commonly involve picking another (actual) word starting with the same sound (e.g., clocktower \u2013 colander), picking another semantically related to the first (e.g., letter \u2013 scroll), or picking one phonetically similar to the intended one (e.g., lane \u2013 late).\n\n\n=### Deaf aphasia=\nThere have been many instances showing that there is a form of aphasia among deaf individuals. Sign languages are, after all, forms of language that have been shown to use the same areas of the brain as verbal forms of language. Mirror neurons become activated when an animal is acting in a particular way or watching another individual act in the same manner. These mirror neurons are important in giving an individual the ability to mimic movements of hands. Broca's area of speech production has been shown to contain several of these mirror neurons resulting in significant similarities of brain activity between sign language and vocal speech communication. People use facial movements to create, what other people perceive, to be faces of emotions. While combining these facial movements with speech, a more full form of language is created which enables the species to interact with a much more complex and detailed form of communication. Sign language also uses these facial movements and emotions along with the primary hand movement way of communicating. These facial movement forms of communication come from the same areas of the brain. When dealing with damages to certain areas of the brain, vocal forms of communication are in jeopardy of severe forms of aphasia. Since these same areas of the brain are being used for sign language, these same, at least very similar, forms of aphasia can show in the Deaf community. Individuals can show a form of Wernicke's aphasia with sign language and they show deficits in their abilities in being able to produce any form of expressions. Broca's aphasia shows up in some people, as well. These individuals find tremendous difficulty in being able to actually sign the linguistic concepts they are trying to express.\n\n\n=### Severity=\nThe severity of the type of aphasia varies depending on the size of the stroke. However, there is much variance between how often one type of severity occurs in certain types of aphasia. For instance, any type of aphasia can range from mild to profound. Regardless of the severity of aphasia, people can make improvements due to spontaneous recovery and treatment in the acute stages of recovery. Additionally, while most studies propose that the greatest outcomes occur in people with severe aphasia when treatment is provided in the acute stages of recovery, Robey (1998) also found that those with severe aphasia are capable of making strong language gains in the chronic stage of recovery as well. This finding implies that persons with aphasia have the potential to have functional outcomes regardless of how severe their aphasia may be. While there is no distinct pattern of the outcomes of aphasia based on severity alone, global aphasia typically makes functional language gains, but may be gradual since global aphasia affects many language areas.\n\n\n## Prevention\n\nAphasia is largely caused by unavoidable instances. However, some precautions can be taken to decrease risk for experiencing one of the two major causes of aphasia: stroke and traumatic brain injury (TBI). To decrease the probability of having an ischemic or hemorrhagic stroke, one should take the following precautions:\n\nExercising regularly\nEating a healthy diet, avoiding cholesterol in particular\nKeeping alcohol consumption low and avoiding tobacco use\nControlling blood pressure\nGoing to the emergency room immediately if you begin to experience unilateral extremity (especially leg) swelling, warmth, redness, and/or tenderness as these are symptoms of a deep vein thrombosis which can lead to a stroke\nTo prevent aphasia due to traumatic injury, one should take precautionary measures when engaging in dangerous activities such as:\n\nWearing a helmet when operating a bicycle, motor cycle, ATV, or any other moving vehicle that could potentially be involved in an accident\nWearing a seatbelt when driving or riding in a car\nWearing proper protective gear when playing contact sports, especially American football, rugby, and hockey, or refraining from such activities\nMinimizing anticoagulant use (including aspirin) if at all possible as they increase the risk of hemorrhage after a head injury\nAdditionally, one should always seek medical attention after sustaining head trauma due to a fall or accident. The sooner that one receives medical attention for a traumatic brain injury, the less likely one is to experience long-term or severe effects.\n\n\n## Management\n\nMost acute cases of aphasia recover some or most skills by participating in speech and language therapy. Recovery and improvement can continue for years after the stroke. After the onset of aphasia, there is approximately a six-month period of spontaneous recovery; during this time, the brain is attempting to recover and repair the damaged neurons. Improvement varies widely, depending on the aphasia's cause, type, and severity. Recovery also depends on the person's age, health, motivation, handedness, and educational level.\nSpeech and language therapy that is higher intensity, higher dose or provided over a long duration of time leads to significantly better functional communication, but people might be more likely to drop out of high intensity treatment (up to 15 hours per week). A total of 20\u201350 hours of speech and language therapy is necessary for the best recovery. The most improvement happens when 2\u20135 hours of therapy is provided each week over 4\u20135 days. Recovery is further improved when besides the therapy people practice tasks at home. Speech and language therapy is also effective if it is delivered online through video or by a family member who has been trained by a professional therapist.\nRecovery with therapy is also dependent on the recency of stroke and the age of the person. Receiving therapy within a month after the stroke leads to the greatest improvements. Three or six months after the stroke more therapy will be needed, but symptoms can still be improved. People with aphasia who are younger than 55 years are the most likely to improve, but people older than 75 years can still get better with therapy.\nThere is no one treatment proven to be effective for all types of aphasias. The reason that there is no universal treatment for aphasia is because of the nature of the disorder and the various ways it is presented. Aphasia is rarely exhibited identically, implying that treatment needs to be catered specifically to the individual. Studies have shown that, although there is no consistency on treatment methodology in literature, there is a strong indication that treatment, in general, has positive outcomes. Therapy for aphasia ranges from increasing functional communication to improving speech accuracy, depending on the person's severity, needs and support of family and friends. Group therapy allows individuals to work on their pragmatic and communication skills with other individuals with aphasia, which are skills that may not often be addressed in individual one-on-one therapy sessions. It can also help increase confidence and social skills in a comfortable setting.\nEvidence does not support the use of transcranial direct current stimulation (tDCS) for improving aphasia after stroke. Moderate quality evidence does indicate naming performance improvements for nouns, but not verbs using tDCS\nSpecific treatment techniques include the following:\n\nCopy and recall therapy (CART) \u2013 repetition and recall of targeted words within therapy may strengthen orthographic representations and improve single word reading, writing, and naming\nVisual communication therapy (VIC) \u2013 the use of index cards with symbols to represent various components of speech\nVisual action therapy (VAT) \u2013 typically treats individuals with global aphasia to train the use of hand gestures for specific items\nFunctional communication treatment (FCT) \u2013 focuses on improving activities specific to functional tasks, social interaction, and self-expression\nPromoting aphasic's communicative effectiveness (PACE) \u2013 a means of encouraging normal interaction between people with aphasia and clinicians. In this kind of therapy, the focus is on pragmatic communication rather than treatment itself. People are asked to communicate a given message to their therapists by means of drawing, making hand gestures or even pointing to an object\nMelodic intonation therapy (MIT) \u2013 aims to use the intact melodic/prosodic processing skills of the right hemisphere to help cue retrieval of words and expressive language\nCenteredness Theory Interview (CTI) - Uses client centered goal formation into the nature of current patient interactions as well as future / desired interactions to improve subjective well-being, cognition and communication.\nOther \u2013 i.e., drawing as a way of communicating, trained conversation partners\nSemantic feature analysis (SFA) \u2014 a type of aphasia treatment that targets word-finding deficits \u2014 is based on the theory that neural connections can be strengthened by using related words and phrases that are similar to the target word, to eventually activate the target word in the brain. SFA can be implemented in multiple forms such as verbally, written, using picture cards, etc. The SLP provides prompting questions to the individual with aphasia in order for the person to name the picture provided. Studies show that SFA is an effective intervention for improving confrontational naming.\nMelodic intonation therapy is used to treat non-fluent aphasia and has proved to be effective in some cases. However, there is still no evidence from randomized controlled trials confirming the efficacy of MIT in chronic aphasia. MIT is used to help people with aphasia vocalize themselves through speech song, which is then transferred as a spoken word. Good candidates for this therapy include people who have had left hemisphere strokes, non-fluent aphasias such as Broca's, good auditory comprehension, poor repetition and articulation, and good emotional stability and memory. An alternative explanation is that the efficacy of MIT depends on neural circuits involved in the processing of rhythmicity and formulaic expressions (examples taken from the MIT manual: \"I am fine,\" \"how are you?\" or \"thank you\"); while rhythmic features associated with melodic intonation may engage primarily left-hemisphere subcortical areas of the brain, the use of formulaic expressions is known to be supported by right-hemisphere cortical and bilateral subcortical neural networks.\nSystematic reviews support the effectiveness and importance of partner training. According to the National Institute on Deafness and Other Communication Disorders (NIDCD), involving family with the treatment of an aphasic loved one is ideal for all involved, because while it will no doubt assist in their recovery, it will also make it easier for members of the family to learn how best to communicate with them.\nWhen a person's speech is insufficient, different kinds of augmentative and alternative communication could be considered such as alphabet boards, pictorial communication books, specialized software for computers or apps for tablets or smartphones.\nWhen addressing Wernicke's aphasia, according to Bakheit et al. (2007), the lack of awareness of the language impairments, a common characteristic of Wernicke's aphasia, may affect the rate and extent of therapy outcomes. Robey (1998) determined that at least 2 hours of treatment per week is recommended for making significant language gains. Spontaneous recovery may cause some language gains, but without speech-language therapy, the outcomes can be half as strong as those with therapy.\nWhen addressing Broca's aphasia, better outcomes occur when the person participates in therapy, and treatment is more effective than no treatment for people in the acute period. Two or more hours of therapy per week in acute and post-acute stages produced the greatest results. High-intensity therapy was most effective, and low-intensity therapy was almost equivalent to no therapy.\nPeople with global aphasia are sometimes referred to as having irreversible aphasic syndrome, often making limited gains in auditory comprehension, and recovering no functional language modality with therapy. With this said, people with global aphasia may retain gestural communication skills that may enable success when communicating with conversational partners within familiar conditions. Process-oriented treatment options are limited, and people may not become competent language users as readers, listeners, writers, or speakers no matter how extensive therapy is. However, people's daily routines and quality of life can be enhanced with reasonable and modest goals. After the first month, there is limited to no healing to language abilities of most people. There is a grim prognosis, leaving 83% who were globally aphasic after the first month that will remain globally aphasic at the first year. Some people are so severely impaired that their existing process-oriented treatment approaches offer no signs of progress, and therefore cannot justify the cost of therapy.\nPerhaps due to the relative rareness of conduction aphasia, few studies have specifically studied the effectiveness of therapy for people with this type of aphasia. From the studies performed, results showed that therapy can help to improve specific language outcomes. One intervention that has had positive results is auditory repetition training. Kohn et al. (1990) reported that drilled auditory repetition training related to improvements in spontaneous speech, Francis et al. (2003) reported improvements in sentence comprehension, and Kalinyak-Fliszar et al. (2011) reported improvements in auditory-visual short-term memory.\n\n\n### Individualized service delivery\nIntensity of treatment should be individualized based on the recency of stroke, therapy goals, and other specific characteristics such as age, size of lesion, overall health status, and motivation. Each individual reacts differently to treatment intensity and is able to tolerate treatment at different times post-stroke. Intensity of treatment after a stroke should be dependent on the person's motivation, stamina, and tolerance for therapy.\n\n\n## Outcomes\n\nIf the symptoms of aphasia last longer than two or three months after a stroke, a complete recovery is unlikely. However, it is important to note that some people continue to improve over a period of years and even decades. Improvement is a slow process that usually involves both helping the individual and family understand the nature of aphasia and learning compensatory strategies for communicating.\nAfter a traumatic brain injury (TBI) or cerebrovascular accident (CVA), the brain undergoes several healing and re-organization processes, which may result in improved language function. This is referred to as spontaneous recovery. Spontaneous recovery is the natural recovery the brain makes without treatment, and the brain begins to reorganize and change in order to recover. There are several factors that contribute to a person's chance of recovery caused by stroke, including stroke size and location. Age, sex, and education have not been found to be very predictive. There is also research pointing to damage in the left hemisphere healing more effectively than the right.\nSpecific to aphasia, spontaneous recovery varies among affected people and may not look the same in everyone, making it difficult to predict recovery.\nThough some cases of Wernicke's aphasia have shown greater improvements than more mild forms of aphasia, people with Wernicke's aphasia may not reach as high a level of speech abilities as those with mild forms of aphasia.\n\n\n## Prevalence\n\nAphasia affects about two million people in the U.S. and 250,000 people in Great Britain. Nearly 180,000 people acquire the disorder every year in the U.S., 170,000 due to stroke. Any person of any age can develop aphasia, given that it is often caused by a traumatic injury. However, people who are middle aged and older are the most likely to acquire aphasia, as the other etiologies are more likely at older ages. For example, approximately 75% of all strokes occur in individuals over the age of 65. Strokes account for most documented cases of aphasia: 25% to 40% of people who survive a stroke develop aphasia as a result of damage to the language-processing regions of the brain.\n\n\n## History\n\nThe first recorded case of aphasia is from an Egyptian papyrus, the Edwin Smith Papyrus, which details speech problems in a person with a traumatic brain injury to the temporal lobe.\nDuring the second half of the 19th century, aphasia was a major focus for scientists and philosophers who were working in the beginning stages of the field of psychology.\nIn medical research, speechlessness was described as an incorrect prognosis, and there was no assumption that underlying language complications existed. Broca and his colleagues were some of the first to write about aphasia, but Wernicke was the first credited to have written extensively about aphasia being a disorder that contained comprehension difficulties. Despite claims of who reported on aphasia first, it was F.J. Gall that gave the first full description of aphasia after studying wounds to the brain, as well as his observation of speech difficulties resulting from vascular lesions. A recent book on the entire history of aphasia is available (Reference: Tesak, J. & Code, C. (2008) Milestones in the History of Aphasia: Theories and Protagonists. Hove, East Sussex: Psychology Press).\n\n\n### Etymology\nAphasia is from Greek a- (\"without\", negative prefix) + ph\u00e1sis (\u03c6\u03ac\u03c3\u03b9\u03c2, \"speech\").\nThe word aphasia comes from the word \u1f00\u03c6\u03b1\u03c3\u03af\u03b1 aphasia, in Ancient Greek, which means \"speechlessness\", derived from \u1f04\u03c6\u03b1\u03c4\u03bf\u03c2 aphatos, \"speechless\" from \u1f00- a-, \"not, un\" and \u03c6\u03b7\u03bc\u03af phemi, \"I speak\".\n\n\n## Further research\n\nResearch is currently being done using functional magnetic resonance imaging (fMRI) to witness the difference in how language is processed in normal brains vs aphasic brains. This will help researchers to understand exactly what the brain must go through in order to recover from Traumatic Brain Injury (TBI) and how different areas of the brain respond after such an injury.\nAnother intriguing approach being tested is that of drug therapy. Research is in progress that will hopefully uncover whether or not certain drugs might be used in addition to speech-language therapy in order to facilitate recovery of proper language function. It's possible that the best treatment for Aphasia might involve combining drug treatment with therapy, instead of relying on one over the other.\nOne other method being researched as a potential therapeutic combination with speech-language therapy is brain stimulation. One particular method, Transcranial Magnetic Stimulation (TMS), alters brain activity in whatever area it happens to stimulate, which has recently led scientists to wonder if this shift in brain function caused by TMS might help people re-learn language. Another type of external brain stimulation is transcranial Direct Current Stimulation (tDCS), but existing research has not shown it to be useful for improving aphasia after a stroke.\n\n\n## See also\n\n\n\n## Notes\n\n\n\n## References\n\n\n\n## External links\n\n",
      "vector_count": 1,
      "vector_offset": 3
    },
    {
      "format": "md",
      "id": "PSY0004_Attention",
      "metadata": {
        "source_format": "md",
        "source_path": "/tmp/corpus_split_psychology/shard_000/PSY0004_Attention.md",
        "source_relpath": "PSY0004_Attention.md"
      },
      "path": "PSY0004_Attention.md",
      "text": "# Attention\n\nAttention or focus, is the concentration of awareness on some phenomenon to the exclusion of other stimuli. It is the selective concentration on discrete information, either subjectively or objectively. William James (1890) wrote that \"Attention is the taking possession by the mind, in clear and vivid form, of one out of what seem several simultaneously possible objects or trains of thought. Focalization, concentration, of consciousness are of its essence.\" Attention has also been described as the allocation of limited cognitive processing resources. Attention is manifested by an attentional bottleneck, in terms of the amount of data the brain can process each second; for example, in human vision, less than 1% of the visual input data stream of 1MByte/sec can enter the bottleneck, leading to inattentional blindness.\nAttention remains a crucial area of investigation within education, psychology, neuroscience, cognitive neuroscience, and neuropsychology. Areas of active investigation involve determining the source of the sensory cues and signals that generate attention, the effects of these sensory cues and signals on the tuning properties of sensory neurons, and the relationship between attention and other behavioral and cognitive processes, which may include working memory and psychological vigilance. A relatively new body of research, which expands upon earlier research within psychopathology, is investigating the diagnostic symptoms associated with traumatic brain injury and its effects on attention. Attention also varies across cultures. For example, people from cultures that center around collectivism pay greater attention to the big picture in the image given to them, rather than specific elements of the image. On the other hand, those involved in more individualistic cultures tend to pay greater attention to the most noticeable portion of the image.\nThe relationships between attention and consciousness are complex enough that they have warranted philosophical exploration. Such exploration is both ancient and continually relevant, as it can have effects in fields ranging from mental health and the study of disorders of consciousness to artificial intelligence and its domains of research.\n\n\n## Contemporary definition and research\n\nPrior to the founding of psychology as a scientific discipline, attention was studied in the field of philosophy. Thus, many of the discoveries in the field of attention were made by philosophers. Psychologist John B. Watson calls Juan Luis Vives the father of modern psychology because, in his book De Anima et Vita (The Soul and Life), he was the first to recognize the importance of empirical investigation.  In his work on memory, Vives found that the more closely one attends to stimuli, the better they will be retained.\nBy the 1990s, psychologists began using positron emission tomography (PET) and later functional magnetic resonance imaging (fMRI) to image the brain while monitoring tasks involving attention. Considering this expensive equipment was generally only available in hospitals, psychologists sought cooperation with neurologists. Psychologist Michael Posner (then already renowned for his influential work on visual selective attention) and neurologist Marcus Raichle pioneered brain imaging studies of selective attention. Their results soon sparked interest from the neuroscience community, which until then had been focused on monkey brains. With the development of these technological innovations, neuroscientists became interested in this type of research that combines sophisticated experimental paradigms from cognitive psychology with these new brain imaging techniques. Although the older technique of electroencephalography (EEG) had long been used to study the brain activity underlying selective attention by cognitive psychophysiologists, the ability of the newer techniques to measure precisely localized activity inside the brain generated renewed interest by a wider community of researchers. A growing body of such neuroimaging research has identified a frontoparietal attention network which appears to be responsible for control of attention.\nA definition of a psychological construct forms a research approach to its study. In scientific works, attention often coincides and substitutes the notion of intentionality due to the extent of semantic uncertainty in the linguistic explanations of these notions' definitions. Intentionality has in turn been defined as \"the power of minds to be about something: to represent or to stand for things, properties and states of affairs\". Although these two psychological constructs (attention and intentionality) appear to be defined by similar terms, they are different notions. To clarify the definition of attention, it would be correct to consider the origin of this notion to review the meaning of the term given to it when the experimental study on attention was initiated. It is thought that the experimental approach began with famous experiments with a 4 x 4 matrix of sixteen randomly chosen letters \u2013 the experimental paradigm that informed Wundt's theory of attention. Wundt interpreted the experimental outcome introducing the meaning of attention as \"that psychical process, which is operative in the clear perception of the narrow region of the content of consciousness.\" These experiments showed the physical limits of attention threshold, which were 3-6 letters observing the matrix during 1/10 s of their exposition. \"We shall call the entrance into the large region of consciousness - apprehension, and the elevation into the focus of attention - apperception.\" Wundt's theory of attention postulated one of the main features of this notion that attention is an active, voluntary process realized during a certain time. In contrast, neuroscience research shows that intentionality may emerge instantly, even unconsciously; research reported to register neuronal correlates of an intentional act that preceded this conscious act (also see shared intentionality). Therefore, while intentionality is a mental state (\u201cthe power of the mind to be about something\u201d, arising even unconsciously), the description of the construct of attention should be understood in the dynamical sense as the ability to elevate the clear perception of the narrow region of the content of consciousness and to keep in mind this state for a time. The attention threshold would be the period of minimum time needed for employing perception to clearly apprehend the scope of intention. From this perspective, a scientific approach to attention is relevant when it considers the difference between these two concepts (first of all, between their statical and dynamical statuses).\nThe growing body of literature shows empirical evidence that attention is conditioned by the number of elements and the duration of exposition. Decades of research on subitizing have supported Wundt's findings about the limits of a human ability to concentrate awareness on a task. Latvian prof. Sandra Mihailova and prof. Igor Val Danilov drew an essential conclusion from the Wundtian approach to the study of attention: the scope of attention is related to cognitive development. As the mind grasps more details about an event, it also increases the number of reasonable combinations within that event, enhancing the probability of better understanding its features and particularity. For example, three items in the focal point of consciousness have six possible combinations (3 factorial), and four items have 24 (4 factorial) combinations. This number of combinations becomes significantly prominent in the case of a focal point with six items with 720 possible combinations (6 factorial). Empirical evidence suggests that the scope of attention in young children develops from two items in the focal point at age up to six months to five or more items in the focal point at age about five years. As follows from the most recent studies in relation to teaching activities in school, \u201cattention\u201d should be understood as \u201cthe state of concentration of an individual's consciousness on the process of selecting by his own psyche the information he requires and on the process of choosing an algorithm for response actions, which involves the intensification of sensory and intellectual activities\u201d.\n\n\n## Selective and visual\n\n\nIn cognitive psychology there are at least two models which describe how visual attention operates. These models may be considered metaphors which are used to describe internal processes and to generate hypotheses that are falsifiable. Generally speaking, visual attention is thought to operate as a two-stage process. In the first stage, attention is distributed uniformly over the external visual scene and processing of information is performed in parallel. In the second stage, attention is concentrated to a specific area of the visual scene (i.e., it is focused), and processing is performed in a serial fashion.\nThe first of these models to appear in the literature is the spotlight model. The term \"spotlight\" was inspired by the work of William James, who described attention as having a focus, a margin, and a fringe. The focus is an area that extracts information from the visual scene with a high-resolution, the geometric center of which being where visual attention is directed. Surrounding the focus is the fringe of attention, which extracts information in a much more crude fashion (i.e., low-resolution). This fringe extends out to a specified area, and the cut-off is called the margin.\nThe second model is called the zoom-lens model and was first introduced in 1986. This model inherits all properties of the spotlight model (i.e., the focus, the fringe, and the margin), but it has the added property of changing in size. This size-change mechanism was inspired by the zoom lens one might find on a camera, and any change in size can be described by a trade-off in the efficiency of processing. The zoom-lens of attention can be described in terms of an inverse trade-off between the size of focus and the efficiency of processing: because attention resources are assumed to be fixed, then it follows that the larger the focus is, the slower processing will be of that region of the visual scene, since this fixed resource will be distributed over a larger area. It is thought that the focus of attention can subtend a minimum of 1\u00b0 of visual angle, however the maximum size has not yet been determined.\nA significant debate emerged in the last decade of the 20th century in which Treisman's 1993 Feature Integration Theory (FIT) was compared to Duncan and Humphrey's 1989 attentional engagement theory (AET). FIT posits that \"objects are retrieved from scenes by means of selective spatial attention that picks out objects' features, forms feature maps, and integrates those features that are found at the same location into forming objects.\" Treismans's theory is based on a two-stage process to help solve the binding problem of attention. These two stages are the preattentive stage and the focused attention stage.\n\nPreattentive Stage: The unconscious detection and separation of features of an item (color, shape, size). Treisman suggests that this happens early in cognitive  processing and that individuals are not aware of the occurrence due to the counter intuitiveness of separating a whole into its part. Evidence shows that preattentive focuses are accurate due to illusory conjunctions.\nFocused Attention Stage: The combining of all feature identifiers to perceive all parts as one whole. This is possible through prior knowledge and cognitive mapping. When an item is seen within a known location and has features that people have knowledge of, then prior knowledge will help bring features all together to make sense of what is perceived. The case of R.M's damage to his parietal lobe, also known as Balint's syndrome, shows the incorporation of focused attention and combination of features in the role of attention.\nThrough sequencing these steps, parallel and serial search is better exhibited through the formation of conjunctions of objects. Conjunctive searches, according to Treismans, are done through both stages in order to create selective and focused attention on an object, though Duncan and Humphrey would disagree. Duncan and Humphrey's AET understanding of attention maintained that \"there is an initial pre-attentive parallel phase of perceptual segmentation and analysis that encompasses all of the visual items present in a scene. At this phase, descriptions of the objects in a visual scene are generated into structural units; the outcome of this parallel phase is a multiple-spatial-scale structured representation. Selective attention intervenes after this stage to select information that will be entered into visual short-term memory.\" The contrast of the two theories placed a new emphasis on the separation of visual attention tasks alone and those mediated by supplementary cognitive processes. As Rastophopoulos summarizes the debate: \"Against Treisman's FIT, which posits spatial attention as a necessary condition for detection of objects, Humphreys argues that visual elements are encoded and bound together in an initial parallel phase without focal attention, and that attention serves to select among the objects that result from this initial grouping.\"\n\n\n## Neuropsychological model\n\nIn the twentieth century, the pioneering research of Lev Vygotsky and Alexander Luria led to the three-part model of neuropsychology defining the working brain as being represented by three co-active processes listed as Attention, Memory, and Activation. A.R. Luria published his well-known book The Working Brain in 1973 as a concise adjunct volume to his previous 1962 book Higher Cortical Functions in Man. In this volume, Luria summarized his three-part global theory of the working brain as being composed of three constantly co-active processes which he described as the; (1) Attention system, (2) Mnestic (memory) system, and (3) Cortical activation system. The two books together are considered by Homskaya's account as \"among Luria's major works in neuropsychology, most fully reflecting all the aspects (theoretical, clinical, experimental) of this new discipline.\" The product of the combined research of Vygotsky and Luria have determined a large part of the contemporary understanding and definition of attention as it is understood at the start of the 21st century.\n\n\n## Multitasking and divided attention\n\n\nMultitasking can be defined as the attempt to perform two or more tasks simultaneously; however, research shows that when multitasking, people make more mistakes or perform their tasks more slowly.  Attention must be divided among all of the component tasks to perform them.  In divided attention, individuals attend or give attention to multiple sources of information at once or perform more than one task at the same time.\nOlder research involved looking at the limits of people performing simultaneous tasks like reading stories, while listening and writing something else, or listening to two separate messages through different ears (i.e., dichotic listening).  Generally, classical research into attention investigated the ability of people to learn new information when there were multiple tasks to be performed, or to probe the limits of our perception (cf. Donald Broadbent).  There is also older literature on people's performance on multiple tasks performed simultaneously, such as driving a car while tuning a radio or driving while being on the phone.\nThe vast majority of current research on human multitasking is based on performance of doing two tasks simultaneously, usually that involves driving while performing another task, such as texting, eating, or even speaking to passengers in the vehicle, or with a friend over a cellphone.  This research reveals that the human attentional system has limits for what it can process: driving performance is worse while engaged in other tasks; drivers make more mistakes, brake harder and later, get into more accidents, veer into other lanes, and/or are less aware of their surroundings when engaged in the previously discussed tasks.\nThere has been little difference found between speaking on a hands-free cell phone or a hand-held cell phone, which suggests that it is the strain of attentional system that causes problems, rather than what the driver is doing with his or her hands.  While speaking with a passenger is as cognitively demanding as speaking with a friend over the phone, passengers are able to change the conversation based upon the needs of the driver.  For example, if traffic intensifies, a passenger may stop talking to allow the driver to navigate the increasingly difficult roadway; a conversation partner over a phone would not be aware of the change in environment.\nThere have been multiple theories regarding divided attention. One, conceived by cognitive scientist Daniel Kahneman, explains that there is a single pool of attentional resources that can be freely divided among multiple tasks. This model seems oversimplified, however, due to the different modalities (e.g., visual, auditory, verbal) that are perceived. When the two simultaneous tasks use the same modality, such as listening to a radio station and writing a paper, it is much more difficult to concentrate on both because the tasks are likely to interfere with each other. The specific modality model was theorized by cognitive psychologists David Navon and Daniel Gopher in 1979. However, more recent research using well controlled dual-task paradigms points at the importance of tasks.\nAs an alternative, resource theory has been proposed as a more accurate metaphor for explaining divided attention on complex tasks. Resource theory states that as each complex task is automatized, performing that task requires less of the individual's limited-capacity attentional resources. Other variables play a part in our ability to pay attention to and concentrate on many tasks at once. These include, but are not limited to, anxiety, arousal, task difficulty, and skills.\n\n\n## Simultaneous\n\nSimultaneous attention is a type of attention, classified by attending to multiple events at the same time. Simultaneous attention is demonstrated by children in Indigenous communities, who learn through this type of attention to their surroundings. Simultaneous attention is present in the ways in which children of indigenous backgrounds interact both with their surroundings and with other individuals. Simultaneous attention requires focus on multiple simultaneous activities or occurrences. This differs from multitasking, which is characterized by alternating attention and focus between multiple activities, or halting one activity before switching to the next.\nSimultaneous attention involves uninterrupted attention to several activities occurring at the same time. Another cultural practice that may relate to simultaneous attention strategies is coordination within a group. Indigenous heritage toddlers and caregivers in San Pedro were observed to frequently coordinate their activities with other members of a group in ways parallel to a model of simultaneous attention, whereas middle-class European-descent families in the U.S. would move back and forth between events. Research concludes that children with close ties to Indigenous American roots have a high tendency to be especially wide, keen observers. This points to a strong cultural difference in attention management.\n\n\n## Alternative topics and discussions\n\n\n\n### Overt and covert orienting\nAttention may be differentiated into \"overt\" versus \"covert\" orienting.\nOvert orienting is the act of selectively attending to an item or location over others by moving the eyes to point in that direction. Overt orienting can be directly observed in the form of eye movements. Although overt eye movements are quite common, there is a distinction that can be made between two types of eye movements; reflexive and controlled. Reflexive movements are commanded by the superior colliculus of the midbrain. These movements are fast and are activated by the sudden appearance of stimuli. In contrast, controlled eye movements are commanded by areas in the frontal lobe. These movements are slow and voluntary.\nCovert orienting is the act of mentally shifting one's focus without moving one's eyes. Simply, it is changes in attention that are not attributable to overt eye movements. Covert orienting has the potential to affect the output of perceptual processes by governing attention to particular items or locations (for example, the activity of a V4 neuron whose receptive field lies on an attended stimuli will be enhanced by covert attention) but does not influence the information that is processed by the senses. Researchers often use \"filtering\" tasks to study the role of covert attention of selecting information. These tasks often require participants to observe a number of stimuli, but attend to only one. The current view is that visual covert attention is a mechanism for quickly scanning the field of view for interesting locations. This shift in covert attention is linked to eye movement circuitry that sets up a slower saccade to that location.\nThere are studies that suggest the mechanisms of overt and covert orienting may not be controlled separately and independently as previously believed. Central mechanisms that may control covert orienting, such as the parietal lobe, also receive input from subcortical centres involved in overt orienting. In support of this, general theories of attention actively assume bottom-up (reflexive) processes and top-down (voluntary) processes converge on a common neural architecture, in that they control both covert and overt attentional systems.  For example, if individuals attend to the right hand corner field of view, movement of the eyes in that direction may have to be actively suppressed.\nCovert attention has been argued to reflect the existence of processes \"programming explicit ocular movement\". However, this has been questioned on the grounds that N2, \"a neural measure of covert attentional allocation\u2014does not always precede eye movements\". However, the researchers acknowledge, \"it may be impossible to definitively rule out the possibility that some kind of shift of covert attention precedes every shift of overt attention\".\n\n\n### Exogenous and endogenous orienting\nOrienting attention is vital and can be controlled through external (exogenous) or internal (endogenous) processes.  However, comparing these two processes is challenging because external signals do not operate completely exogenously, but will only summon attention and eye movements if they are important to the subject.\nExogenous (from Greek exo, meaning \"outside\", and genein, meaning \"to produce\") orienting is frequently described as being under control of a stimulus.  Exogenous orienting is considered to be reflexive and automatic and is caused by a sudden change in the periphery. This often results in a reflexive saccade. Since exogenous cues are typically presented in the periphery, they are referred to as peripheral cues. Exogenous orienting can even be observed when individuals are aware that the cue will not relay reliable, accurate information about where a target is going to occur. This means that the mere presence of an exogenous cue will affect the response to other stimuli that are subsequently presented in the cue's previous location.\nSeveral studies have investigated the influence of valid and invalid cues. They concluded that valid peripheral cues benefit performance, for instance when the peripheral cues are brief flashes at the relevant location before the onset of a visual stimulus. Psychologists Michael Posner and Yoav Cohen (1984) noted a reversal of this benefit takes place when the interval between the onset of the cue and the onset of the target is longer than about 300 ms. The phenomenon  of valid cues producing longer reaction times than invalid cues is called inhibition of return.\nEndogenous (from Greek endo, meaning \"within\" or \"internally\") orienting is the intentional allocation of attentional resources to a predetermined location or space. Simply stated, endogenous orienting occurs when attention is oriented according to an observer's goals or desires, allowing the focus of attention to be manipulated by the demands of a task. In order to have an effect, endogenous cues must be processed by the observer and acted upon purposefully. These cues are frequently referred to as central cues. This is because they are typically presented at the center of a display, where an observer's eyes are likely to be fixated. Central cues, such as an arrow or digit presented at fixation, tell observers to attend to a specific location.\nWhen examining differences between exogenous and endogenous orienting, some researchers suggest that there are four differences between the two kinds of cues: \n\nexogenous orienting is less affected by cognitive load than endogenous orienting;\nobservers are able to ignore endogenous cues but not exogenous cues;\nexogenous cues have bigger effects than endogenous cues; and\nexpectancies about cue validity and predictive value affects endogenous orienting more than exogenous orienting.\nThere exist both overlaps and differences in the areas of the brain that are responsible for endogenous and exogenous orientating. Another approach to this discussion has been covered under the topic heading of \"bottom-up\" versus \"top-down\" orientations to attention. Researchers of this school have described two different aspects of how the mind focuses attention to items present in the environment. The first aspect is called bottom-up processing, also known as stimulus-driven attention or exogenous attention. These describe attentional processing which is driven by the properties of the objects themselves. Some processes, such as motion or a sudden loud noise, can attract our attention in a pre-conscious, or non-volitional way. We attend to them whether we want to or not. These aspects of attention are thought to involve parietal and temporal cortices, as well as the brainstem. More recent experimental evidence support the idea that the primary visual cortex creates a bottom-up saliency map, which is received by  the superior colliculus in the midbrain area to  guide attention or gaze shifts.\nThe second aspect is called top-down processing, also known as goal-driven, endogenous attention, attentional control or executive attention. This aspect of our attentional orienting is under the control of the person who is attending. It is mediated primarily by the frontal cortex and basal ganglia as one of the executive functions. Research has shown that it is related to other aspects of the executive functions, such as working memory, and conflict resolution and inhibition.\n\n\n### Influence of processing load\nA \"hugely influential\" theory regarding selective attention is the perceptual load theory, which states that there are two mechanisms that affect attention: cognitive and perceptual. The perceptual mechanism considers the subject's ability to perceive or ignore stimuli, both task-related and non task-related. Studies show that if there are many stimuli present (especially if they are task-related), it is much easier to ignore the non-task related stimuli, but if there are few stimuli the mind will perceive the irrelevant stimuli as well as the relevant. The cognitive mechanism refers to the actual processing of the stimuli. Studies regarding this showed that the ability to process stimuli decreased with age, meaning that younger people were able to perceive more stimuli and fully process them, but were likely to process both relevant and irrelevant information, while older people could process fewer stimuli, but usually processed only relevant information.\nSome people can process multiple stimuli, e.g. trained Morse code operators have been able to copy 100% of a message while carrying on a meaningful conversation.  This relies on the reflexive response due to \"overlearning\" the skill of morse code reception/detection/transcription so that it is an autonomous function requiring no specific attention to perform. This overtraining of the brain comes as the \"practice of a skill [surpasses] 100% accuracy,\" allowing the activity to become autonomic, while your mind has room to process other actions simultaneously.\nBased on the primary role of the perceptual load theory, assumptions regarding its functionality surrounding that attentional resources are that of limited capacity which signify the need for all of the attentional resources to be used. This performance, however, is halted when put hand in hand with accuracy and reaction time (RT). This limitation arises through the measurement of literature when obtaining outcomes for scores. This affects both cognitive and perceptual attention because there is a lack of measurement surrounding distributions of temporal and spatial attention. Only a concentrated amount of attention on how effective one is completing the task and how long they take is being analyzed making a more redundant analysis on overall cognition of being able to process multiple stimuli through perception.\n\n\n### Clinical model\nAttention is best described as the sustained focus of cognitive resources on information while filtering or ignoring extraneous information. Attention is a very basic function that often is a precursor to all other neurological/cognitive functions. As is frequently the case, clinical models of attention differ from investigation models. One of the most used models for the evaluation of attention in patients with very different neurologic pathologies is the model of Sohlberg and Mateer. This hierarchic model is based in the recovering of attention processes of brain damage patients after coma. Five different kinds of activities of growing difficulty are described in the model; connecting with the activities those patients could do as their recovering process advanced.\n\nFocused attention: The ability to respond discretely to specific sensory stimuli.\nSustained attention (vigilance and concentration): The ability to maintain a consistent behavioral response during continuous and repetitive activity.\nSelective attention: The ability to maintain a behavioral or cognitive set in the face of distracting or competing stimuli. Therefore, it incorporates the notion of \"freedom from distractibility.\"\nAlternating attention: The ability of mental flexibility that allows individuals to shift their focus of attention and move between tasks having different cognitive requirements.\nDivided attention: This refers to the ability to respond simultaneously to multiple tasks or multiple task demands.\nThis model has been shown to be very useful in evaluating attention in very different pathologies, correlates strongly with daily difficulties and is especially helpful in designing stimulation programs such as attention process training, a rehabilitation program for neurological patients of the same authors.\n\n\n## Other descriptors for types of attention\n\nMindfulness:  Mindfulness has been conceptualized as a clinical model of attention.  Mindfulness practices are clinical interventions that emphasize training attention functions.\nVigilant attention: Remaining focused on a non-arousing stimulus or uninteresting task for a sustained period is far more difficult than attending to arousing stimuli and interesting tasks, and requires a specific type of attention called 'vigilant attention'. Thereby, vigilant attention is the ability to give sustained attention to a  stimulus or  task that might ordinarily be insufficiently engaging to prevent our attention being distracted by other stimuli or tasks.\n\n\n### Neural correlates\nMost experiments show that one neural correlate of attention is enhanced firing. If a neuron has a different response to a stimulus when an animal is not attending to a stimulus, versus when the animal does attend to the stimulus, then the neuron's response will be enhanced even if the physical characteristics of the stimulus remain the same.\nIn a 2007 review, Professor Eric Knudsen describes a more general model which identifies four core processes of attention, with working memory at the center:\n\nWorking memory temporarily stores information for detailed analysis.\nCompetitive selection is the process that determines which information gains access to working memory.\nThrough top-down sensitivity control, higher cognitive processes can regulate signal intensity in information channels that compete for access to working memory, and thus give them an advantage in the process of competitive selection. Through top-down sensitivity control, the momentary content of working memory can influence the selection of new information, and thus mediate voluntary control of attention in a recurrent loop (endogenous attention).\nBottom-up saliency filters automatically enhance the response to infrequent stimuli, or stimuli of instinctive or learned biological relevance (exogenous attention).\nNeurally, at different hierarchical levels spatial maps can enhance or inhibit activity in sensory areas, and induce orienting behaviors like eye movement.\n\nAt the top of the hierarchy, the frontal eye fields (FEF) and the dorsolateral prefrontal cortex contain a retinocentric spatial map. Microstimulation in the FEF induces monkeys to make a saccade to the relevant location. Stimulation at levels too low to induce a saccade will nonetheless enhance cortical responses to stimuli located in the relevant area.\nAt the next lower level, a variety of spatial maps are found in the parietal cortex. In particular, the lateral intraparietal area (LIP) contains a saliency map and is interconnected both with the FEF and with sensory areas.\nExogenous attentional guidance in humans and monkeys is by a bottom-up saliency map in the primary visual cortex.  In lower vertebrates, this saliency map is more likely in the superior colliculus (optic tectum).\nCertain automatic responses that influence attention, like orienting to a highly salient stimulus, are mediated subcortically by the superior colliculi.\nAt the neural network level, it is thought that processes like lateral inhibition mediate the process of competitive selection.\nIn many cases attention produces changes in the EEG. Many animals, including humans, produce gamma waves (40\u201360 Hz) when focusing attention on a particular object or activity.\nAnother commonly used model for the attention system has been put forth by researchers such as Michael Posner. He divides attention into three functional components: alerting, orienting, and executive attention that can also interact and influence each other.\n\nAlerting is the process involved in becoming and staying attentive toward the surroundings. It appears to exist in the frontal and parietal lobes of the right hemisphere, and is modulated by norepinephrine.\nOrienting is the directing of attention to a specific stimulus.\nExecutive attention is used when there is a conflict between multiple attention cues. It is essentially the same as the central executive in Baddeley's model of working memory. The Eriksen flanker task has shown that the executive control of attention may take place in the anterior cingulate cortex\n\n\n### Cultural variation\nChildren appear to develop patterns of attention related to the cultural practices of their families, communities, and the institutions in which they participate.\nIn 1955, Jules Henry suggested that there are societal differences in sensitivity to signals from many ongoing sources that call for the awareness of several levels of attention simultaneously. He tied his speculation to ethnographic observations of communities in which children are involved in a complex social community with multiple relationships.\nMany Indigenous children in the Americas predominantly learn by observing and pitching in. There are several studies to support that the use of keen attention towards learning is much more common in Indigenous Communities of North and Central America than in a middle-class European-American setting. This is a direct result of the Learning by Observing and Pitching In model.\nKeen attention is both a requirement and result of learning by observing and pitching-in. Incorporating the children in the community gives them the opportunity to keenly observe and contribute to activities that were not directed towards them. It can be seen from different Indigenous communities and cultures, such as the Mayans of San Pedro, that children can simultaneously attend to multiple events. Most Maya children have learned to pay attention to several events at once in order to make useful observations.\nOne example is simultaneous attention which involves uninterrupted attention to several activities occurring at the same time. Another cultural practice that may relate to simultaneous attention strategies is coordination within a group. San Pedro toddlers and caregivers frequently coordinated their activities with other members of a group in multiway engagements rather than in a dyadic fashion. Research concludes that children with close ties to Indigenous American roots have a high tendency to be especially keen observers.\nThis learning by observing and pitching-in model requires active levels of attention management. The child is present while caretakers engage in daily activities and responsibilities such as: weaving, farming, and other skills necessary for survival. Being present allows the child to focus their attention on the actions being performed by their parents, elders, and/or older siblings. In order to learn in this way, keen attention and focus is required. Eventually the child is expected to be able to perform these skills themselves.\nIn one study, it was found that when looking at a picture, Americans focus more on the center figure than Japanese do, especially after 1 second has passed. Japanese individuals spent larger amounts of time focusing on parts in the background. Miyamoto et al. compared pictures of landscapes in Japan and the US, noting that Japanese scenes contained more boundaries and edges than the American ones.\n\n\n### Modelling\nIn the domain of computer vision, efforts have been made to model the mechanism of human attention, especially the bottom-up intentional mechanism and its semantic significance in classification of video contents. Both spatial attention and temporal attention have been incorporated in such classification efforts.\nGenerally speaking, there are two kinds of models to mimic the bottom-up salience mechanism in static images. One is based on the spatial contrast analysis. For example, a center\u2013surround mechanism has been used to define salience across scales, inspired by the putative neural mechanism. It has also been hypothesized that some visual inputs are intrinsically salient in certain background contexts and that these are actually task-independent. This model has established itself as the exemplar for salience detection and consistently used for comparison in the literature; the other kind of model is based on the frequency domain analysis. This method was first proposed by Hou et al.. This method was called SR. Then, the PQFT method was also introduced. Both SR and PQFT only use the phase information. In 2012, the HFT method was introduced, and both the amplitude and the phase information are made use of. The Neural Abstraction Pyramid is a hierarchical recurrent convolutional model, which incorporates bottom-up and top-down flow of information to iteratively interpret images.\n\n\n### Hemispatial neglect\n\nHemispatial neglect, also called unilateral neglect, often occurs when people have damage to the right hemisphere of their brain. This damage often leads to a tendency to ignore the left side of one's body or even the left side of an object that can be seen. Damage to the left side of the brain (the left hemisphere) rarely yields significant neglect of the right side of the body or object in the person's local environments.\nThe effects of spatial neglect, however, may vary and differ depending on what area of the brain was damaged. Damage to different neural substrates can result in different types of neglect. Attention disorders (lateralized and nonlaterized) may also contribute to the symptoms and effects. Much research has asserted that damage to gray matter within the brain results in spatial neglect.\nNew technology has yielded more information, such that there is a large, distributed network of frontal, parietal, temporal, and subcortical brain areas that have been tied to neglect. This network can be related to other research as well; the dorsal attention network is tied to spatial orienting. The effect of damage to this network may result in patients neglecting their left side when distracted about their right side or an object on their right side.\n\n\n### Attention in social contexts\nSocial attention is one special form of attention that involves the allocation of limited processing resources in a social context. Previous studies on social attention often regard how attention is directed toward socially relevant stimuli such as faces and gaze directions of other individuals. In contrast to attending-to-others, a different line of researches has shown that self-related information such as own face and name automatically captures attention and is preferentially processed comparing to other-related information. These contrasting effects between attending-to-others and attending-to-self prompt a synthetic view in a recent Opinion article proposing that social attention operates at two polarizing states: In one extreme, individual tends to attend to the self and prioritize self-related information over others', and, in the other extreme, attention is allocated to other individuals to infer their intentions and desires. Attending-to-self and attending-to-others mark the two ends of an otherwise continuum spectrum of social attention. For a given behavioral context, the mechanisms underlying these two polarities might interact and compete with each other in order to determine a saliency map of social attention that guides our behaviors. An imbalanced competition between these two behavioral and cognitive processes will cause cognitive disorders and neurological symptoms such as autism spectrum disorders and Williams syndrome.\n\n\n### Distracting factors\nAccording to Daniel Goleman's book, Focus: The Hidden Driver of Excellence, there are two types of distracting factors affecting focus \u2013 sensory and emotional.\nA sensory distracting factor would be, for example, while a person is reading this article, they are neglecting the white field surrounding the text.\nAn emotional distracting factor would be when someone is focused on answering an email, and somebody shouts their name. It would be almost impossible to neglect the voice speaking it. Attention is immediately directed toward the source. Positive emotions have also been found to affect attention. Induction of happiness has led to increased response times and an increase in inaccurate responses in the face of irrelevant stimuli. Two possible theories as to why emotions might make one more susceptible to distracting stimuli is that emotions take up too much of one's cognitive resources and make it harder to control your focus of attention. The other theory is that emotions make it harder to filter out distractions, specifically with positive emotions due to a feeling of security.\nAnother distracting factor to attention processes is insufficient sleep. Sleep deprivation is found to impair cognition, specifically performance in divided attention. Divided attention is possibly linked with the circadian processes.\n\n\n### Failure to attend\nInattentional blindness was first introduced in 1998 by Arien Mack and Irvic Rock. Their studies show that when people are focused on specific stimuli, they often miss other stimuli that are clearly present. Though actual blindness is not occurring here, the blindness that happens is due to the perceptual load of what is being attended to. Based on the experiment performed by Mack and Rock, Ula Finch and Nilli Lavie tested participants with a perceptual task. They presented subjects with a cross, one arm being longer than the other, for 5 trials. On the sixth trial, a white square was added to the top left of the screen. The results conclude that out of 10 participants, only 2 (20%) actually saw the square. This would suggest that when a higher focus was attended to the length of the crossed arms, the more likely someone would altogether miss an object that was in plain sight.\nChange blindness was first tested by Rensink and coworkers in 1997. Their studies show that people have difficulty detecting changes from scene to scene due to the intense focus on one thing, or lack of attention overall. This was tested by Rensink through a presentation of a picture, and then a blank field, and then the same picture but with an item missing. The results showed that the pictures had to be alternated back and forth a good number of times for participants to notice the difference. This idea is greatly portrayed in films that have continuity errors. Many people do not pick up on differences when in reality, the changes tend to be significant.\n\n\n## History of the study\n\n\n\n### Philosophical period\nPsychologist Daniel E. Berlyne credits the first extended treatment of attention to philosopher Nicolas Malebranche in his work \"The Search After Truth\". \"Malebranche held that we have access to ideas, or mental representations  of the external world, but not direct access to the world itself.\" Thus in order to keep these ideas organized, attention is necessary. Otherwise we will confuse these ideas. Malebranche writes in \"The Search After Truth\", \"because it often happens that the understanding has only confused and imperfect perceptions of things, it is truly a cause of our errors.... It is therefore necessary to look for means to keep our perceptions from being confused and imperfect.  And, because, as everyone knows, there is nothing that makes them clearer and more distinct than attentiveness, we must try to find the means to become more attentive than we are\". According to Malebranche, attention is crucial to understanding and keeping thoughts organized.\nPhilosopher Gottfried Wilhelm Leibniz introduced the concept of apperception to this philosophical approach to attention.  Apperception refers to \"the process by which new experience is assimilated to and transformed by the residuum of past experience of an individual to form a new whole.\" Apperception is required for a perceived event to become a conscious event. Leibniz emphasized a reflexive involuntary view of attention known as exogenous orienting. However, there is also endogenous orienting which is voluntary and directed attention. Philosopher Johann Friedrich Herbart agreed with Leibniz's view of apperception; however, he expounded on it in by saying that new experiences had to be tied to ones already existing in the mind. Herbart was also the first person to stress the importance of applying mathematical modeling to the study of psychology.\nThroughout the philosophical era, various thinkers made significant contributions to the field of attention studies, beginning with research on the extent of attention and how attention is directed. In the beginning of the 19th century, it was thought that people were not able to attend to more than one stimulus at a time. However, with research contributions by Sir William Hamilton, 9th Baronet this view was changed.  Hamilton proposed a view of attention that likened its capacity to holding marbles. You can only hold a certain number of marbles at a time before it starts to spill over. His view states that we can attend to more than one stimulus at once. William Stanley Jevons later expanded this view and stated that we can attend to up to four items at a time.\n\n\n### 1860\u20131909\nThis period of attention research took the focus from conceptual findings to experimental testing. It also involved psychophysical methods that allowed measurement of the relation between physical stimulus properties and the psychological perceptions of them. This period covers the development of attentional research from the founding of psychology to 1909.\nWilhelm Wundt introduced the study of attention to the field of psychology. Wundt measured mental processing speed by likening it to differences in stargazing measurements. Astronomers in this time would measure the time it took for stars to travel.  Among these measurements when astronomers recorded the times, there were personal differences in calculation. These different readings resulted in different reports from each astronomer. To correct for this, a personal equation was developed. Wundt applied this to mental processing speed. Wundt realized that the time it takes to see the stimulus of the star and write down the time was being called an \"observation error\" but actually was the time it takes to switch voluntarily one's attention from one stimulus to another. Wundt called his school of psychology voluntarism. It was his belief that psychological processes can only be understood in terms of goals and consequences.\nFranciscus Donders used mental chronometry to study attention and it was considered a major field of intellectual inquiry by authors such as Sigmund Freud. Donders and his students conducted the first detailed investigations of the speed of mental processes. Donders measured the time required to identify a stimulus and to select a motor response. This was the time difference between stimulus discrimination and response initiation.  Donders also formalized the subtractive method which states that the time for a particular process can be estimated by adding that process to a task and taking the difference in reaction time between the two tasks. He also differentiated between three types of reactions: simple reaction, choice reaction, and go/no-go reaction.\nHermann von Helmholtz also contributed to the field of attention relating to the extent of attention. Von Helmholtz stated that it is possible to focus on one stimulus and still perceive or ignore others.  An example of this is being able to focus on the letter u in the word house and still perceiving the letters h, o, s, and e.\nOne major debate in this period was whether it was possible to attend to two things at once (split attention). Walter Benjamin described this experience as \"reception in a state of distraction.\" This disagreement could only be resolved through experimentation.\nIn 1890, William James, in his textbook The Principles of Psychology, remarked:\n\nEveryone knows what attention is. It is the taking possession by the mind, in clear and vivid form, of one out of what seem several simultaneously possible objects or trains of thought. Focalization, concentration, of consciousness are of its essence. It implies withdrawal from some things in order to deal effectively with others, and is a condition which has a real opposite in the confused, dazed, scatterbrained state which in French is called distraction, and Zerstreutheit in German.\nJames differentiated between sensorial attention and intellectual attention. Sensorial attention is when attention is directed to objects of sense, stimuli that are physically present. Intellectual attention is attention directed to ideal or represented objects; stimuli that are not physically present. James also distinguished between immediate or derived attention: attention to the present versus to something not physically present. According to James, attention has five major effects. Attention works to make us perceive, conceive, distinguish, remember, and shorten reactions time.\n\n\n### 1910\u20131949\nDuring this period, research in attention waned and interest in behaviorism flourished, leading some to believe, like Ulric Neisser, that in this period, \"There was no research on attention\". However, Jersild published very important work on \"Mental Set and Shift\" in 1927. He stated, \"The fact of mental set is primary in all conscious activity.  The same stimulus may evoke any one of a large number of responses depending upon the contextual setting in which it is placed\". This research found that the time to complete a list was longer for mixed lists than for pure lists. For example, if a list was names of animals versus a list of the same size with names of animals, books, makes and models of cars, and types of fruits, it takes longer to process the second list. This is task switching.\nIn 1931, Telford discovered the psychological refractory period.  The stimulation of neurons is followed by a refractory phase during which neurons are less sensitive to stimulation. In 1935 John Ridley Stroop developed the Stroop Task which elicited the Stroop Effect. Stroop's task showed that irrelevant stimulus information can have a major impact on performance.  In this task, subjects were to look at a list of colors. This list of colors had each color typed in a color different from the actual text. For example, the word Blue would be typed in Orange, Pink in Black, and so on.\nExample:  Blue Purple Red Green Purple Green\nSubjects were then instructed to say the name of the ink color and ignore the text. It took 110 seconds to complete a list of this type compared to 63 seconds to name the colors when presented in the form of solid squares. The naming time nearly doubled in the presence  of conflicting color words, an effect known as the Stroop Effect.\n\n\n### 1950\u20131974\nIn the 1950s, research psychologists renewed their interest in attention when the dominant epistemology shifted from positivism (i.e., behaviorism) to realism during what has come to be known as the \"cognitive revolution\". The cognitive revolution admitted unobservable cognitive processes like attention as legitimate objects of scientific study.\n\nModern research on attention began with the analysis of the \"cocktail party problem\" by Colin Cherry in 1953. At a cocktail party how do people select the conversation that they are listening to and ignore the rest? This problem is at times called \"focused attention\", as opposed to \"divided attention\". Cherry performed a number of experiments which became known as dichotic listening and were extended by Donald Broadbent and others. In a typical experiment, subjects would use a set of headphones to listen to two streams of words in different ears and selectively attend to one stream. After the task, the experimenter would question the subjects about the content of the unattended stream.\nBroadbent's Filter Model of Attention states that information is held in a pre-attentive temporary store, and only sensory events that have some physical feature in common are selected to pass into the limited capacity processing system.  This implies that the meaning of unattended messages is not identified. Also, a significant amount of time is required to shift the filter from one channel to another. Experiments by Gray and Wedderburn and later Anne Treisman pointed out various problems in Broadbent's early model and eventually led to the Deutsch\u2013Norman model in 1968. In this model, no signal is filtered out, but all are processed to the point of activating their stored representations in memory. The point at which attention becomes \"selective\" is when one of the memory representations is selected for further processing. At any time, only one can be selected, resulting in the attentional bottleneck.\nThis debate became known as the early-selection vs. late-selection models. In the early selection models (first proposed by Donald Broadbent), attention shuts down (in Broadbent's model) or attenuates (in Treisman's refinement) processing in the unattended ear before the mind can analyze its semantic content. In the late selection models (first proposed by J. Anthony Deutsch and Diana Deutsch), the content in both ears is analyzed semantically, but the words in the unattended ear cannot access consciousness. Lavie's perceptual load theory, however, \"provided elegant solution to\" what had once been a \"heated debate\".\n\n\n## See also\n\n\n\n## References\n\n\n\n## Further reading\n\n",
      "vector_count": 1,
      "vector_offset": 4
    }
  ]
}